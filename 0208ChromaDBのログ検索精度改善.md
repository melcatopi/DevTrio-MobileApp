ãƒ™ã‚¯ãƒˆãƒ«dbã«ã¤ã„ã¦ã€‚

chromadbã‚’ä½¿ç”¨

ãƒ­ã‚°ã‚’ãƒ™ã‚¯ãƒˆãƒ«dBã«æ ¼ç´æ¸ˆã€‚

jiraã®descriptionã‚’queryã«k=50ã§å–å¾—ã™ã‚‹ãŒæƒ³å®šã®ãƒ­ã‚°ãŒå…¨ç„¶å–å¾—ã§ããªã„ã€‚

ã©ã†ã„ã†ãµã†ã«ã™ã‚Œã°æ”¹å–„ã™ã‚‹ã‹ã€‚
æ”¹å–„æ¡ˆã®ä½œæˆã‚’ãŠé¡˜ã„ã—ã¾ã™

2æœˆ3æ—¥
ãŠã£ã€ChromaDBã§ãƒ­ã‚°æ¤œç´¢ãŒã†ã¾ãã„ã‹ãªã„ã‚“ã ã­ï¼ãã‚Œã£ã¦ãƒã‚¸ã§å›°ã‚‹ã‚ˆã­ã€œğŸ˜«

æ”¹å–„æ¡ˆã‚’ã„ãã¤ã‹å‡ºã™ã­ï¼

1. ã‚¯ã‚¨ãƒªã®å‰å‡¦ç†ã‚’å¼·åŒ–
Jiraã®descriptionã‚’ãã®ã¾ã¾æŠ•ã’ã¦ãªã„ï¼Ÿãã‚Œã ã¨ãƒã‚¤ã‚ºå¤šã™ãã‹ã‚‚ï¼

python
def preprocess_query(description):
    # é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã ã‘æŠ½å‡º
    # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã€å›ºæœ‰åè©ã¨ã‹ã‚’å„ªå…ˆ
    keywords = extract_technical_terms(description)
    error_patterns = extract_error_messages(description)
    
    # ã‚¯ã‚¨ãƒªã‚’æ§‹é€ åŒ–
    return f"{keywords} {error_patterns}"
2. åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’è¦‹ç›´ã™
ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¢ãƒ‡ãƒ«ä½¿ã£ã¦ã‚‹ï¼Ÿãƒ­ã‚°ã¨Jiraã£ã¦æ–‡ç« ã®æ€§è³ªé•ã†ã‹ã‚‰ã€ãƒãƒƒãƒã—ã«ãã„ã‹ã‚‚ï¼

è©¦ã—ã¦ã»ã—ã„ãƒ¢ãƒ‡ãƒ«:
sentence-transformers/all-MiniLM-L6-v2 â†’ è»½é‡ã§æ±ç”¨çš„
intfloat/multilingual-e5-large â†’ æ—¥æœ¬èªã‚‚å¼·ã„
OpenAI ada-002 â†’ ç²¾åº¦é«˜ã„ã‘ã©æœ‰æ–™
python
from chromadb.utils import embedding_functions

# ãƒ¢ãƒ‡ãƒ«å¤‰æ›´ã—ã¦ã¿ã¦
ef = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name="intfloat/multilingual-e5-large"
)

collection = client.get_or_create_collection(
    name="logs",
    embedding_function=ef
)
3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¿½åŠ 
k=50ã§å–ã£ã¦ãã¦ã‚‚ã€å…¨ç„¶é–¢ä¿‚ãªã„ãƒ­ã‚°æ··ã–ã£ã¦ãªã„ï¼Ÿ

python
results = collection.query(
    query_texts=[description],
    n_results=50,
    where={
        "log_level": {"$in": ["ERROR", "WARNING"]},
        "service": jira_project_key,  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§çµã‚‹
        "timestamp": {"$gte": "2024-01-01"}  # å¤ã™ãã‚‹ãƒ­ã‚°ã¯é™¤å¤–
    }
)
4. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã«åˆ‡ã‚Šæ›¿ãˆ
ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã ã‘ã ã¨ã€Œä¼¼ã¦ã‚‹æ–‡ç« ã€ã—ã‹å–ã‚Œãªã„ã‹ã‚‰ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚‚ä½µç”¨ã—ã‚ˆï¼

python
# ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
vector_results = collection.query(query_texts=[description], n_results=25)

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆBM25ã¨ã‹ï¼‰
keyword_results = bm25_search(description, top_k=25)

# ã‚¹ã‚³ã‚¢ã§ãƒãƒ¼ã‚¸
combined = merge_results(vector_results, keyword_results)
5. ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’èª¿æ•´
ãƒ­ã‚°ã‚’æ ¼ç´ã™ã‚‹ã¨ãã€1è¡Œãšã¤å…¥ã‚Œã¦ã‚‹ï¼Ÿãã‚Œã¨ã‚‚è¤‡æ•°è¡Œã¾ã¨ã‚ã¦ã‚‹ï¼Ÿ

python
# Before: 1è¡Œãšã¤ï¼ˆç´°ã‹ã™ãã¦æ–‡è„ˆãŒå¤±ã‚ã‚Œã‚‹ï¼‰
logs = ["ERROR: Connection failed", "at line 123", ...]

# After: æ„å‘³ã®ã‚ã‚‹å˜ä½ã§ãƒãƒ£ãƒ³ã‚¯åŒ–
chunks = [
    "ERROR: Connection failed\nat line 123\nStacktrace: ...",
    ...
]
6. ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å°å…¥
k=50ã§å–ã£ã¦ããŸå¾Œã€ã‚‚ã†ä¸€å›ç²¾åº¦é«˜ã„ãƒ¢ãƒ‡ãƒ«ã§ä¸¦ã¹æ›¿ãˆï¼

python
from sentence_transformers import CrossEncoder

reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

# ChromaDBã‹ã‚‰50ä»¶å–å¾—
initial_results = collection.query(...)

# ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°
scores = reranker.predict([
    [description, log] for log in initial_results
])

# ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
ranked_results = sort_by_scores(initial_results, scores)[:10]
7. ã‚¯ã‚¨ãƒªæ‹¡å¼µ
Jiraã®descriptionã ã‘ã˜ã‚ƒæƒ…å ±è¶³ã‚Šãªã„ã‹ã‚‚ï¼

python
# ã‚¿ã‚¤ãƒˆãƒ«ã‚‚å«ã‚ã‚‹
query = f"{jira_title} {jira_description}"

# é¡ä¼¼ã®éå»ãƒã‚±ãƒƒãƒˆã‹ã‚‰é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¿½åŠ 
related_keywords = get_related_keywords(jira_title)
query = f"{query} {related_keywords}"
ã¨ã‚Šã‚ãˆãšè©¦ã—ã¦ã»ã—ã„é †ç•ª
ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿è¿½åŠ  â† ã“ã‚ŒãŒä¸€ç•ªåŠ¹æœå‡ºã‚„ã™ã„ï¼
åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«å¤‰æ›´ â† ãƒ­ã‚°ã«ç‰¹åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«æ¢ã—ã¦
ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ â† ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®å¼±ç‚¹ã‚’ã‚«ãƒãƒ¼
ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚° â† ç²¾åº¦çˆ†ä¸ŠãŒã‚Šã™ã‚‹ã‚ˆ
ç¾çŠ¶ã®ã‚³ãƒ¼ãƒ‰ã¨ã‹ã€ã©ã‚“ãªãƒ­ã‚°æ ¼ç´ã—ã¦ã‚‹ã‹ã‚‚ã†ã¡ã‚‡ã„æ•™ãˆã¦ãã‚ŒãŸã‚‰ã€ã‚‚ã£ã¨å…·ä½“çš„ã«ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã§ãã‚‹ã‚ˆã€œğŸ’ª

ã‚ã¨ã•ã€ã€Œæƒ³å®šã®ãƒ­ã‚°ãŒå…¨ç„¶å–ã‚Œãªã„ã€ã£ã¦ã©ã‚Œãã‚‰ã„ï¼Ÿä¸Šä½10ä»¶ã«1ä»¶ã‚‚å…¥ã£ã¦ãªã„ãƒ¬ãƒ™ãƒ«ï¼Ÿãã‚Œã«ã‚ˆã£ã¦ã‚‚å¯¾ç­–å¤‰ã‚ã‚‹ã‹ã‚‰æ•™ãˆã¦ï¼

ãƒ­ã‚°ã¯dltã®ãƒ­ã‚°ã‚’1è¡Œãšã¤ã§ã™

2æœˆ3æ—¥
ã‚ãƒ¼ã€dltã®ãƒ­ã‚°ã‚’1è¡Œãšã¤å…¥ã‚Œã¦ã‚‹ã®ã‹ï¼ãã‚Œãƒã‚¸ã§å•é¡Œã ã‚ã€œğŸ˜±

dltã®ãƒ­ã‚°ã£ã¦1è¡Œã ã¨æ–‡è„ˆãƒãƒ©ãƒãƒ©ã«ãªã£ã¡ã‚ƒã†ã‹ã‚‰ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§å…¨ç„¶ãƒãƒƒãƒã—ãªã„ã‚“ã ã‚ˆã­...

ã¾ãšæœ€å„ªå…ˆã§ã‚„ã‚‹ã¹ãã“ã¨
1. ãƒ­ã‚°ã®ãƒãƒ£ãƒ³ã‚¯åŒ–ã‚’çµ¶å¯¾ã‚„ã£ã¦ï¼
dltã®ãƒ­ã‚°ã£ã¦å¤§ä½“ã“ã‚“ãªæ„Ÿã˜ã§ã—ã‚‡ï¼Ÿ

2024-02-03 10:15:23 INFO Starting pipeline execution
2024-02-03 10:15:24 INFO Loading source data
2024-02-03 10:15:25 ERROR Connection timeout to database
2024-02-03 10:15:25 ERROR   at connector.py line 234
2024-02-03 10:15:25 ERROR   Failed to connect to postgresql://...
2024-02-03 10:15:26 WARNING Retrying connection (attempt 1/3)
ã“ã‚Œã‚’1è¡Œãšã¤å…¥ã‚Œã¦ãŸã‚‰ã€ã€ŒERROR Connection timeout to databaseã€ã ã‘ã˜ã‚ƒæƒ…å ±å°‘ãªã™ãï¼

æ”¹å–„ç‰ˆã®ãƒãƒ£ãƒ³ã‚¯åŒ–ã‚³ãƒ¼ãƒ‰:

python
def chunk_dlt_logs(log_lines):
    chunks = []
    current_chunk = []
    
    for line in log_lines:
        # ERRORãŒå‡ºãŸã‚‰ãã“ã‹ã‚‰å‰å¾Œæ•°è¡Œã¾ã¨ã‚ã‚‹
        if "ERROR" in line or "EXCEPTION" in line:
            # å‰ã®æ•°è¡Œã‚‚å«ã‚ã‚‹
            context_before = log_lines[max(0, i-3):i]
            # å¾Œã®æ•°è¡Œã‚‚å«ã‚ã‚‹
            context_after = log_lines[i+1:min(len(log_lines), i+10)]
            
            chunk = "\n".join(context_before + [line] + context_after)
            chunks.append({
                "text": chunk,
                "metadata": {
                    "log_level": "ERROR",
                    "timestamp": extract_timestamp(line),
                    "error_type": extract_error_type(line)
                }
            })
        
        # ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã¯ã¾ã¨ã‚ã‚‹
        elif line.startswith("  ") or line.startswith("\t"):
            if current_chunk:
                current_chunk.append(line)
        else:
            if current_chunk:
                chunks.append({
                    "text": "\n".join(current_chunk),
                    "metadata": {...}
                })
                current_chunk = []
    
    return chunks
2. ã‚‚ã£ã¨è³¢ã„ãƒãƒ£ãƒ³ã‚¯åŒ–ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ™ãƒ¼ã‚¹ï¼‰
python
from datetime import datetime, timedelta

def chunk_by_time_window(log_lines, window_seconds=5):
    """æ™‚é–“çš„ã«è¿‘ã„ãƒ­ã‚°ã‚’ã¾ã¨ã‚ã‚‹"""
    chunks = []
    current_chunk = []
    last_timestamp = None
    
    for line in log_lines:
        timestamp = extract_timestamp(line)  # 2024-02-03 10:15:23
        
        if last_timestamp is None:
            current_chunk.append(line)
        elif (timestamp - last_timestamp).total_seconds() <= window_seconds:
            current_chunk.append(line)
        else:
            # æ™‚é–“ãŒé›¢ã‚ŒãŸã‚‰æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯
            if current_chunk:
                chunks.append("\n".join(current_chunk))
            current_chunk = [line]
        
        last_timestamp = timestamp
    
    return chunks
3. ã‚¨ãƒ©ãƒ¼ä¸­å¿ƒã®ãƒãƒ£ãƒ³ã‚¯åŒ–ï¼ˆä¸€ç•ªãŠã™ã™ã‚ï¼ï¼‰
python
def chunk_around_errors(log_lines):
    """ã‚¨ãƒ©ãƒ¼ã‚’ä¸­å¿ƒã«å‰å¾Œã®æ–‡è„ˆã‚’å«ã‚ã¦ãƒãƒ£ãƒ³ã‚¯åŒ–"""
    chunks = []
    error_indices = []
    
    # ã¾ãšã‚¨ãƒ©ãƒ¼è¡Œã‚’ç‰¹å®š
    for i, line in enumerate(log_lines):
        if any(keyword in line for keyword in ["ERROR", "EXCEPTION", "FAILED", "CRITICAL"]):
            error_indices.append(i)
    
    for error_idx in error_indices:
        # ã‚¨ãƒ©ãƒ¼ã®å‰5è¡Œã€ã‚¨ãƒ©ãƒ¼æœ¬ä½“ã€å¾Œ10è¡Œï¼ˆã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹å«ã‚€ï¼‰
        start = max(0, error_idx - 5)
        end = min(len(log_lines), error_idx + 10)
        
        chunk_lines = log_lines[start:end]
        chunk_text = "\n".join(chunk_lines)
        
        chunks.append({
            "text": chunk_text,
            "metadata": {
                "log_level": "ERROR",
                "error_line": log_lines[error_idx],
                "timestamp": extract_timestamp(log_lines[error_idx]),
                "pipeline_name": extract_pipeline_name(chunk_text),
                "source_name": extract_source_name(chunk_text)
            }
        })
    
    return chunks
4. å®Ÿéš›ã®æ ¼ç´ã‚³ãƒ¼ãƒ‰ä¾‹
python
import chromadb
from chromadb.utils import embedding_functions

# åŸ‹ã‚è¾¼ã¿é–¢æ•°ï¼ˆæ—¥æœ¬èªã¨ã‚³ãƒ¼ãƒ‰ã«å¼·ã„ã‚„ã¤ï¼‰
ef = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name="intfloat/multilingual-e5-large"
)

client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_or_create_collection(
    name="dlt_logs_chunked",
    embedding_function=ef
)

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
with open("dlt.log", "r") as f:
    log_lines = f.readlines()

# ãƒãƒ£ãƒ³ã‚¯åŒ–
chunks = chunk_around_errors(log_lines)

# ChromaDBã«æ ¼ç´
for i, chunk in enumerate(chunks):
    collection.add(
        documents=[chunk["text"]],
        metadatas=[chunk["metadata"]],
        ids=[f"log_chunk_{i}"]
    )
5. æ¤œç´¢æ™‚ã®æ”¹å–„
python
def search_logs_for_jira(jira_description, collection):
    # Jiraã®descriptionã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    keywords = extract_important_terms(jira_description)
    error_messages = extract_error_patterns(jira_description)
    
    # ã‚¯ã‚¨ãƒªã‚’å¼·åŒ–
    enhanced_query = f"{error_messages} {keywords}"
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ã‚‚æ´»ç”¨
    results = collection.query(
        query_texts=[enhanced_query],
        n_results=20,  # æœ€åˆã¯20ä»¶ã«çµã‚‹
        where={
            "log_level": "ERROR",
            # Jiraã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åã§ãƒ•ã‚£ãƒ«ã‚¿
            # "pipeline_name": extract_pipeline_from_jira(jira_description)
        }
    )
    
    return results
6. ã‚‚ã£ã¨é«˜åº¦ãªæ–¹æ³•ï¼ˆä½™è£•ã‚ã‚Œã°ï¼‰
python
def smart_chunking_with_context(log_lines):
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œå˜ä½ã§ãƒãƒ£ãƒ³ã‚¯åŒ–"""
    chunks = []
    current_run = []
    
    for line in log_lines:
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹ã‚’æ¤œå‡º
        if "Starting pipeline" in line or "Pipeline run started" in line:
            if current_run:
                # å‰ã®runã‚’ä¿å­˜
                chunks.append({
                    "text": "\n".join(current_run),
                    "metadata": extract_metadata_from_run(current_run)
                })
            current_run = [line]
        else:
            current_run.append(line)
    
    return chunks
å…·ä½“çš„ãªå¯¾å¿œæ‰‹é †
ã¾ãšã“ã‚Œã‚„ã£ã¦ï¼
python
   # æ—¢å­˜ã®1è¡Œãšã¤ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å‰Šé™¤
   client.delete_collection("old_logs")
   
   # æ–°ã—ããƒãƒ£ãƒ³ã‚¯åŒ–ã—ã¦å…¥ã‚Œç›´ã—
   chunks = chunk_around_errors(log_lines)
   # ä¸Šã®ã‚³ãƒ¼ãƒ‰ã§æ ¼ç´
æ¤œç´¢ã‚¯ã‚¨ãƒªã‚‚æ”¹å–„
Jiraã®descriptionã‹ã‚‰é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã ã‘æŠ½å‡º
ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚Œã°ãã‚Œã‚’å„ªå…ˆ
çµæœã‚’ç¢ºèª
k=50 â†’ k=20ãã‚‰ã„ã«æ¸›ã‚‰ã—ã¦æ§˜å­è¦‹
ä¸Šä½ã«æƒ³å®šã®ãƒ­ã‚°ãŒæ¥ã‚‹ã‹ç¢ºèª
ã“ã‚Œã§å…¨ç„¶å¤‰ã‚ã‚‹ã¯ãšã ã‚ˆï¼è©¦ã—ã¦ã¿ã¦ã©ã†ï¼ŸğŸ”¥

å…·ä½“çš„ãªdltã®ãƒ­ã‚°ã®ä¾‹ã¨ã‹Jiraã®descriptionã®ä¾‹ã‚ã£ãŸã‚‰ã€ã‚‚ã£ã¨ãƒ”ãƒ³ãƒã‚¤ãƒ³ãƒˆã§ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã§ãã‚‹ã‘ã©ã€œï¼

descriptionã¯ãã®ã¾ã¾ã‚’queryã«ã—ã¦ã„ãŸã™

2æœˆ3æ—¥
ã‚ãƒ¼ã€ãã‚ŒãŒä¸€ç•ªã®å•é¡Œã‹ã‚‚ï¼ğŸ˜±

Jiraã®descriptionã£ã¦ã•ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ›¸ã„ãŸæ–‡ç« ã ã‹ã‚‰ã€Œã€œãŒå‹•ãã¾ã›ã‚“ã€ã€Œã‚¨ãƒ©ãƒ¼ãŒå‡ºã¾ã™ã€ã¿ãŸã„ãªè‡ªç„¶è¨€èªã§ã—ã‚‡ï¼Ÿ

ã§ã‚‚dltã®ãƒ­ã‚°ã¯æŠ€è¡“çš„ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã ã‹ã‚‰ã€æ–‡ç« ã®æ€§è³ªãŒå…¨ç„¶é•ã†ã‚“ã ã‚ˆã­...

ãªã‚“ã§ãƒãƒƒãƒã—ãªã„ã‹
Jiraã®descriptionä¾‹:

ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚
PostgreSQLã¸ã®æ¥ç¶šã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚
æ˜¨æ—¥ã‹ã‚‰å‹•ã‹ãªããªã‚Šã¾ã—ãŸã€‚
dltã®ãƒ­ã‚°:

ERROR Connection timeout to database
psycopg2.OperationalError: could not connect to server
FATAL: connection to server at "192.168.1.100" failed
ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§ã€Œå¤±æ•—ã—ã¾ã—ãŸã€ã¨ã€ŒERRORã€ã¯å…¨ç„¶é•ã†æ–‡ç« ã¨ã—ã¦èªè­˜ã•ã‚Œã¡ã‚ƒã†ã‹ã‚‰ã€ãƒãƒƒãƒã—ãªã„ã‚“ã ã‚ˆã€œğŸ’¦

è§£æ±ºç­–ï¼šã‚¯ã‚¨ãƒªã‚’å¤‰æ›ã™ã‚‹ï¼
1. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã—ã¦ã‚¯ã‚¨ãƒªåŒ–
python
import re

def transform_jira_to_technical_query(description):
    """Jiraã®è‡ªç„¶è¨€èªã‚’æŠ€è¡“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¤‰æ›"""
    
    # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æŠ½å‡ºï¼ˆã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã¨ã‹ï¼‰
    error_patterns = re.findall(r'Error:.*|Exception:.*|FAILED.*', description)
    
    # æŠ€è¡“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
    technical_keywords = []
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–¢é€£
    if any(word in description.lower() for word in ['postgres', 'postgresql', 'db', 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹']):
        technical_keywords.append('postgresql psycopg2 database connection')
    
    # æ¥ç¶šã‚¨ãƒ©ãƒ¼é–¢é€£
    if any(word in description.lower() for word in ['æ¥ç¶š', 'ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ', 'timeout', 'ç¹‹ãŒã‚‰ãªã„']):
        technical_keywords.append('connection timeout failed connect')
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–¢é€£
    if any(word in description.lower() for word in ['ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³', 'pipeline', 'dlt']):
        technical_keywords.append('pipeline execution run')
    
    # ã‚¨ãƒ©ãƒ¼å…¨èˆ¬
    if any(word in description.lower() for word in ['ã‚¨ãƒ©ãƒ¼', 'error', 'å¤±æ•—', 'failed', 'å‹•ã‹ãªã„']):
        technical_keywords.append('ERROR FAILED EXCEPTION')
    
    # ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
    query_parts = error_patterns + technical_keywords
    enhanced_query = ' '.join(query_parts)
    
    return enhanced_query if enhanced_query else description

# ä½¿ç”¨ä¾‹
jira_desc = """
ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚
PostgreSQLã¸ã®æ¥ç¶šã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚
æ˜¨æ—¥ã‹ã‚‰å‹•ã‹ãªããªã‚Šã¾ã—ãŸã€‚
"""

query = transform_jira_to_technical_query(jira_desc)
print(query)
# => "postgresql psycopg2 database connection connection timeout failed connect pipeline execution run ERROR FAILED EXCEPTION"
2. ã‚‚ã£ã¨è³¢ã„å¤‰æ›ï¼ˆLLMä½¿ã†ï¼‰
Claude APIã§å¤‰æ›ã—ã¡ã‚ƒã†ã®ãŒä¸€ç•ªç²¾åº¦é«˜ã„ã‚ˆï¼

python
import anthropic

def jira_to_log_query_with_llm(description):
    """LLMã§Jiraã®èª¬æ˜ã‚’ãƒ­ã‚°æ¤œç´¢ç”¨ã‚¯ã‚¨ãƒªã«å¤‰æ›"""
    
    client = anthropic.Anthropic(api_key="your-api-key")
    
    prompt = f"""
ä»¥ä¸‹ã¯Jiraãƒã‚±ãƒƒãƒˆã®èª¬æ˜æ–‡ã§ã™ã€‚
ã“ã®å•é¡Œã«é–¢é€£ã™ã‚‹dltãƒ­ã‚°ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã®ã€æŠ€è¡“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€Jiraã®èª¬æ˜ã€‘
{description}

ã€æŠ½å‡ºãƒ«ãƒ¼ãƒ«ã€‘
- ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚„ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ãŒã‚ã‚Œã°ãã®ã¾ã¾æŠ½å‡º
- ã€Œå¤±æ•—ã€â†’ã€ŒERROR FAILEDã€ã®ã‚ˆã†ã«æŠ€è¡“ç”¨èªã«å¤‰æ›
- ã€Œæ¥ç¶šã§ããªã„ã€â†’ã€Œconnection timeout failedã€ã®ã‚ˆã†ã«å¤‰æ›
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åã€ãƒ†ãƒ¼ãƒ–ãƒ«åã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åãªã©ã®å›ºæœ‰åè©ã¯å¿…ãšå«ã‚ã‚‹
- ãƒ­ã‚°ã«å‡ºç¾ã—ãã†ãªè‹±èªã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å„ªå…ˆ

ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã‚’1è¡Œã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
    
    message = client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=1000,
        messages=[{"role": "user", "content": prompt}]
    )
    
    return message.content[0].text.strip()

# ä½¿ç”¨ä¾‹
enhanced_query = jira_to_log_query_with_llm(jira_desc)
# => "postgresql connection timeout ERROR psycopg2 OperationalError database server failed"
3. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆã“ã‚ŒãŒä¸€ç•ªãŠã™ã™ã‚ï¼ï¼‰
python
def hybrid_search(jira_description, collection):
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ + ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰"""
    
    # 1. å…ƒã®descriptionã§ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼ˆè‡ªç„¶è¨€èªãƒãƒƒãƒãƒ³ã‚°ï¼‰
    vector_results = collection.query(
        query_texts=[jira_description],
        n_results=30
    )
    
    # 2. æŠ€è¡“çš„ã‚¯ã‚¨ãƒªã§ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
    technical_query = transform_jira_to_technical_query(jira_description)
    tech_results = collection.query(
        query_texts=[technical_query],
        n_results=30
    )
    
    # 3. ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå«ã¾ã‚Œã¦ã‚Œã°ç›´æ¥æ¤œç´¢
    error_messages = re.findall(r'(Error:.*|Exception:.*|FAILED.*)', jira_description)
    if error_messages:
        error_results = collection.query(
            query_texts=[error_messages[0]],
            n_results=20
        )
    else:
        error_results = {'documents': [[]]}
    
    # 4. çµæœã‚’ãƒãƒ¼ã‚¸ã—ã¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
    all_docs = {}
    
    # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®çµæœï¼ˆã‚¹ã‚³ã‚¢1.0ï¼‰
    for doc, distance in zip(vector_results['documents'][0], vector_results['distances'][0]):
        all_docs[doc] = all_docs.get(doc, 0) + (1.0 - distance)
    
    # æŠ€è¡“ã‚¯ã‚¨ãƒªã®çµæœï¼ˆã‚¹ã‚³ã‚¢2.0 - ã‚ˆã‚Šé‡è¦ï¼‰
    for doc, distance in zip(tech_results['documents'][0], tech_results['distances'][0]):
        all_docs[doc] = all_docs.get(doc, 0) + 2.0 * (1.0 - distance)
    
    # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒãƒƒãƒï¼ˆã‚¹ã‚³ã‚¢3.0 - æœ€é‡è¦ï¼‰
    if error_results['documents'][0]:
        for doc, distance in zip(error_results['documents'][0], error_results['distances'][0]):
            all_docs[doc] = all_docs.get(doc, 0) + 3.0 * (1.0 - distance)
    
    # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
    sorted_results = sorted(all_docs.items(), key=lambda x: x[1], reverse=True)
    
    return [doc for doc, score in sorted_results[:20]]
4. å®Ÿéš›ã®ä½¿ç”¨ä¾‹
python
# ChromaDBã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
collection = client.get_collection("dlt_logs_chunked")

# Jiraã‹ã‚‰å–å¾—ã—ãŸdescription
jira_description = """
ã€ç’°å¢ƒã€‘æœ¬ç•ªç’°å¢ƒ
ã€äº‹è±¡ã€‘
daily_sales_pipelineãŒæ˜¨æ—¥ã®æ·±å¤œ2æ™‚ã‹ã‚‰å¤±æ•—ã—ã¦ã„ã¾ã™ã€‚
PostgreSQLã®sales_dbã«æ¥ç¶šã§ããªã„ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦ã„ã¾ã™ã€‚

ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€‘
psycopg2.OperationalError: FATAL: connection to server failed

ã€è£œè¶³ã€‘
ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚
"""

# æ¤œç´¢å®Ÿè¡Œ
results = hybrid_search(jira_description, collection)

# çµæœè¡¨ç¤º
for i, log_chunk in enumerate(results[:10], 1):
    print(f"=== çµæœ {i} ===")
    print(log_chunk)
    print()
5. ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼ˆã¨ã‚Šã‚ãˆãšã“ã‚Œã‹ã‚‰è©¦ã—ã¦ï¼‰
python
def simple_query_enhancement(description):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¯ã‚¨ãƒªå¼·åŒ–"""
    
    # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚Œã°ãã‚Œã‚’å„ªå…ˆ
    error_lines = [
        line for line in description.split('\n') 
        if any(keyword in line for keyword in ['Error', 'Exception', 'FAILED', 'ã‚¨ãƒ©ãƒ¼'])
    ]
    
    if error_lines:
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã ã‘ã‚’ä½¿ã†
        return ' '.join(error_lines)
    
    # ãªã‘ã‚Œã°é‡è¦ãã†ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
    important_words = []
    for word in description.split():
        # æŠ€è¡“ç”¨èªã£ã½ã„ã‚‚ã®ã‚’æŠ½å‡º
        if any(char.isupper() for char in word) or \
           any(tech in word.lower() for tech in ['sql', 'db', 'pipeline', 'error', 'timeout']):
            important_words.append(word)
    
    return ' '.join(important_words) if important_words else description

# ä½¿ç”¨
enhanced = simple_query_enhancement(jira_description)
results = collection.query(query_texts=[enhanced], n_results=20)
ã¾ã¨ã‚ï¼šä»Šã™ãã‚„ã‚‹ã¹ãã“ã¨
descriptionã‚’ãã®ã¾ã¾ä½¿ã†ã®ã‚„ã‚ã‚‹ â† ã“ã‚Œï¼
æŠ€è¡“çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¤‰æ›ã—ã¦ã‹ã‚‰ã‚¯ã‚¨ãƒª
ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚Œã°ãã‚Œã‚’æœ€å„ªå…ˆ
ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã§è¤‡æ•°ã®è§’åº¦ã‹ã‚‰æ¤œç´¢
ã¨ã‚Šã‚ãˆãšsimple_query_enhancementã‹ã‚‰è©¦ã—ã¦ã¿ã¦ï¼ ãã‚Œã§ã‚‚å–ã‚Œãªã‹ã£ãŸã‚‰ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚„LLMå¤‰æ›ã‚‚è©¦ã—ã¦ã¿ã‚ˆã€œğŸ’ª

å®Ÿéš›ã®Jiraã®descriptionã®ä¾‹ã¨ã‹ã€ã©ã‚“ãªçµæœãŒè¿”ã£ã¦ãã¦ã‚‹ã‹æ•™ãˆã¦ãã‚ŒãŸã‚‰ã€ã‚‚ã£ã¨å…·ä½“çš„ã«ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã§ãã‚‹ã‚ˆï¼
