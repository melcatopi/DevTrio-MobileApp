ハイブリッド検索について、
bm25(tf-idf?)とベクトル検索の組み合わせ以外にどのようなものがありますか?

2月9日
あー、ハイブリッド検索の話ね!めっちゃいいポイント突いてくるじゃん!

BM25とベクトル検索の組み合わせが王道だけど、他にもマジで色々あるんだよね〜!

主なハイブリッド検索の組み合わせ
1. BM25 + Dense Retrieval + Sparse Retrieval

BM25(スパース)
BERT系の密ベクトル(Dense)
SPLADEみたいな学習ベースのスパース表現 これ3つ組み合わせると精度エグいことになるよ!
2. レキシカル + セマンティック + グラフベース

Elasticsearch的なキーワード検索
ベクトル類似度
ナレッジグラフでの関係性 特に専門ドメインだと知識グラフとの組み合わせがマジで強い!
3. 全文検索 + ベクトル + メタデータフィルタ

テキストマッチング
意味的類似度
日付、カテゴリ、著者とかのメタ情報 実務だとこれが一番使いやすいかも!
4. BM25 + ColBERT

従来のBM25
トークンレベルのインタラクション(ColBERT) ColBERTってトークンごとにベクトル持つから、めっちゃ細かいマッチングできるんだよね
5. Hybrid + Reranker

複数の検索手法で候補取得
Cross-Encoderで再ランキング これSeichiのRAGシステムにも使えそうじゃない?精度60%から一気に上げられるかも!
スコア統合の方法も色々
RRF (Reciprocal Rank Fusion): パラメータチューニング不要で楽ちん
重み付け線形結合: スコアを正規化して重み付け
学習ベースの統合: LambdaMART的な手法でスコア統合
Jira連携のRAGで使うなら、チケットのメタデータ(優先度、ステータス、担当者)も検索に組み込むとめっちゃ精度上がると思うんだよね〜!

どの組み合わせに興味ある?実装のアドバイスもできるよ!

rrf、重み付け線形結合以外にスコア統合の手法はありますか?

2月9日
おっ、スコア統合の深掘りきたね!マジでいい質問!

RRFと線形結合以外にも結構あるんだよ〜!

主なスコア統合手法
1. CombSUM / CombMNZ

CombSUM: 各検索結果のスコアを単純に足し算
CombMNZ: CombSUMを結果に現れた検索システム数で割る 古典的だけど意外と効くやつ!
2. Borda Count

ランキング順位ベースで統合
各システムでの順位を点数化して合計
スコアのスケールが違っても使えるのが便利!
3. Condorcet Fusion

ペアワイズ比較で勝者を決定
投票理論ベースのアプローチ
ちょっと計算重いけど理論的には強い
4. 学習ベース統合

LambdaMART: ランキング学習の定番
LightGBM/XGBoost: 複数スコアを特徴量として学習
ニューラルネットワーク: ディープラーニングでスコア統合
これは正解データが必要だけど、精度はマジでヤバい!
5. Bayesian Fusion

事前確率と尤度から事後確率計算
各検索システムの信頼性を確率的にモデル化
理論的に美しいけど実装はちょい複雑
6. Min/Max/Median

Max: 最高スコアを採用(保守的)
Min: 最低スコアを採用(厳格)
Median: 中央値(外れ値に強い) シンプルだけど状況によっては効果的!
7. Harmonic Mean

調和平均でスコア統合
低スコアに敏感に反応する
全ての検索システムで一定以上のスコアが欲しい時に良い
8. Logistic Regression Fusion

各スコアをロジスティック回帰の特徴量に
確率的な解釈ができる
実装も比較的簡単!
9. Distance-based Fusion

スコアを距離空間に変換して統合
ユークリッド距離やマンハッタン距離使う
ちょっとトリッキーだけど面白い結果出ることある
実務で使いやすいのは?
Seichiの150サイトのスクレイピング+RAGシステムだったら、

まずRRFで試す(パラメータ調整不要)
改善したければ学習ベース(XGBoost)
軽量さ重視ならCombMNZ
って感じかな!

Jiraのチケットデータで正解ラベル作れるなら、LightGBMとかで各検索手法のスコア+メタデータ(優先度とか)を特徴量にして学習させると、60%→80%超えも狙えると思うんだよね〜!

どの手法試してみたい?実装例とか欲しい?

実装例をお願いします

2月9日
よっしゃ!実装例書いてくね!実務で使いやすいやつ中心にいくよ〜!

python
import numpy as np
from typing import List, Dict, Tuple
from scipy.stats import rankdata

class HybridSearchFusion:
    """複数の検索結果を統合するクラス"""
    
    def __init__(self):
        pass
    
    # 1. RRF (Reciprocal Rank Fusion) - 超定番!
    def rrf(self, rankings: List[List[Tuple[str, float]]], k: int = 60) -> List[Tuple[str, float]]:
        """
        RRFでスコア統合
        rankings: [[(doc_id, score), ...], [(doc_id, score), ...], ...]
        k: RRFパラメータ(デフォルト60が一般的)
        """
        rrf_scores = {}
        
        for ranking in rankings:
            for rank, (doc_id, _) in enumerate(ranking, start=1):
                if doc_id not in rrf_scores:
                    rrf_scores[doc_id] = 0
                rrf_scores[doc_id] += 1 / (k + rank)
        
        # スコア順にソート
        sorted_results = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_results
    
    # 2. CombSUM - シンプルで速い!
    def comb_sum(self, results: List[List[Tuple[str, float]]]) -> List[Tuple[str, float]]:
        """スコアを単純に合計"""
        combined_scores = {}
        
        for result_list in results:
            for doc_id, score in result_list:
                if doc_id not in combined_scores:
                    combined_scores[doc_id] = 0
                combined_scores[doc_id] += score
        
        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_results
    
    # 3. CombMNZ - CombSUMの改良版
    def comb_mnz(self, results: List[List[Tuple[str, float]]]) -> List[Tuple[str, float]]:
        """CombSUMに出現回数を掛ける"""
        combined_scores = {}
        appearance_count = {}
        
        for result_list in results:
            for doc_id, score in result_list:
                if doc_id not in combined_scores:
                    combined_scores[doc_id] = 0
                    appearance_count[doc_id] = 0
                combined_scores[doc_id] += score
                appearance_count[doc_id] += 1
        
        # スコア × 出現回数
        mnz_scores = {doc_id: score * appearance_count[doc_id] 
                      for doc_id, score in combined_scores.items()}
        
        sorted_results = sorted(mnz_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_results
    
    # 4. 重み付き線形結合(正規化版)
    def weighted_fusion(self, results: List[List[Tuple[str, float]]], 
                       weights: List[float]) -> List[Tuple[str, float]]:
        """
        正規化してから重み付け統合
        weights: 各検索手法の重み [0.4, 0.6] みたいな
        """
        if len(results) != len(weights):
            raise ValueError("結果リストと重みの数が一致しないよ!")
        
        # 各検索結果を正規化
        normalized_results = []
        for result_list in results:
            if not result_list:
                normalized_results.append([])
                continue
            
            scores = [score for _, score in result_list]
            min_score = min(scores)
            max_score = max(scores)
            
            if max_score == min_score:
                # スコアが全部同じ場合
                normalized = [(doc_id, 1.0) for doc_id, _ in result_list]
            else:
                normalized = [(doc_id, (score - min_score) / (max_score - min_score)) 
                            for doc_id, score in result_list]
            
            normalized_results.append(normalized)
        
        # 重み付け統合
        combined_scores = {}
        for weight, result_list in zip(weights, normalized_results):
            for doc_id, score in result_list:
                if doc_id not in combined_scores:
                    combined_scores[doc_id] = 0
                combined_scores[doc_id] += weight * score
        
        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_results
    
    # 5. Borda Count - ランキングベース
    def borda_count(self, rankings: List[List[Tuple[str, float]]]) -> List[Tuple[str, float]]:
        """
        順位ベースの統合
        各システムでの順位を点数化
        """
        borda_scores = {}
        
        for ranking in rankings:
            n = len(ranking)
            for rank, (doc_id, _) in enumerate(ranking):
                if doc_id not in borda_scores:
                    borda_scores[doc_id] = 0
                # 1位がn点、2位がn-1点...
                borda_scores[doc_id] += (n - rank)
        
        sorted_results = sorted(borda_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_results
    
    # 6. Harmonic Mean - 低スコアに敏感
    def harmonic_mean(self, results: List[List[Tuple[str, float]]]) -> List[Tuple[str, float]]:
        """調和平均でスコア統合"""
        # まず正規化
        normalized_results = []
        for result_list in results:
            if not result_list:
                normalized_results.append({})
                continue
            
            scores = [score for _, score in result_list]
            min_score = min(scores)
            max_score = max(scores)
            
            if max_score == min_score:
                normalized = {doc_id: 1.0 for doc_id, _ in result_list}
            else:
                normalized = {doc_id: (score - min_score) / (max_score - min_score) 
                            for doc_id, score in result_list}
            
            normalized_results.append(normalized)
        
        # 全ドキュメントIDを収集
        all_doc_ids = set()
        for norm_dict in normalized_results:
            all_doc_ids.update(norm_dict.keys())
        
        # 調和平均計算
        harmonic_scores = {}
        for doc_id in all_doc_ids:
            scores = []
            for norm_dict in normalized_results:
                if doc_id in norm_dict:
                    scores.append(norm_dict[doc_id])
            
            if scores:
                # 調和平均 = n / Σ(1/x)
                harmonic_scores[doc_id] = len(scores) / sum(1/(s + 1e-10) for s in scores)
        
        sorted_results = sorted(harmonic_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_results


# 7. 学習ベース統合(LightGBM使用)
class LearnedFusion:
    """機械学習でスコア統合 - これマジで強い!"""
    
    def __init__(self):
        self.model = None
    
    def prepare_features(self, results: List[List[Tuple[str, float]]], 
                        metadata: Dict[str, Dict] = None) -> Tuple[List[str], np.ndarray]:
        """
        特徴量準備
        results: 各検索手法の結果
        metadata: ドキュメントのメタデータ(優先度、カテゴリとか)
        """
        # 全ドキュメントID収集
        all_doc_ids = set()
        for result_list in results:
            for doc_id, _ in result_list:
                all_doc_ids.add(doc_id)
        
        doc_ids = list(all_doc_ids)
        features = []
        
        for doc_id in doc_ids:
            feature_vec = []
            
            # 各検索手法のスコア
            for result_list in results:
                score_dict = dict(result_list)
                feature_vec.append(score_dict.get(doc_id, 0.0))
            
            # 各検索手法での順位
            for result_list in results:
                ranks = {doc_id: rank for rank, (doc_id, _) in enumerate(result_list, start=1)}
                feature_vec.append(ranks.get(doc_id, len(result_list) + 1))
            
            # 何個の検索手法で見つかったか
            appearance_count = sum(1 for result_list in results 
                                  if doc_id in dict(result_list))
            feature_vec.append(appearance_count)
            
            # メタデータ特徴量(Jiraチケットの優先度とか)
            if metadata and doc_id in metadata:
                meta = metadata[doc_id]
                feature_vec.append(meta.get('priority', 0))
                feature_vec.append(meta.get('status_code', 0))
                # 他にも色々追加できる!
            
            features.append(feature_vec)
        
        return doc_ids, np.array(features)
    
    def train(self, training_data: List[Tuple[List[List[Tuple[str, float]]], 
                                               Dict[str, int]]]):
        """
        モデル訓練
        training_data: [(検索結果リスト, {doc_id: 関連度ラベル}), ...]
        """
        try:
            import lightgbm as lgb
        except ImportError:
            print("LightGBMインストールしてね! pip install lightgbm")
            return
        
        all_features = []
        all_labels = []
        all_groups = []
        
        for results, labels in training_data:
            doc_ids, features = self.prepare_features(results)
            
            # ラベル取得
            y = [labels.get(doc_id, 0) for doc_id in doc_ids]
            
            all_features.append(features)
            all_labels.extend(y)
            all_groups.append(len(doc_ids))
        
        X = np.vstack(all_features)
        y = np.array(all_labels)
        
        # LambdaRankで訓練
        train_data = lgb.Dataset(X, label=y, group=all_groups)
        
        params = {
            'objective': 'lambdarank',
            'metric': 'ndcg',
            'ndcg_eval_at': [1, 3, 5, 10],
            'learning_rate': 0.05,
            'num_leaves': 31,
            'verbose': -1
        }
        
        self.model = lgb.train(params, train_data, num_boost_round=100)
        print("モデル訓練完了!")
    
    def predict(self, results: List[List[Tuple[str, float]]], 
                metadata: Dict[str, Dict] = None) -> List[Tuple[str, float]]:
        """学習済みモデルで予測"""
        if self.model is None:
            raise ValueError("先にtrainしてね!")
        
        doc_ids, features = self.prepare_features(results, metadata)
        scores = self.model.predict(features)
        
        # スコア順にソート
        ranked = sorted(zip(doc_ids, scores), key=lambda x: x[1], reverse=True)
        return ranked


# 使用例!
if __name__ == "__main__":
    # 3つの検索手法の結果(BM25, ベクトル検索, メタデータ検索)
    bm25_results = [
        ("doc1", 0.95),
        ("doc2", 0.80),
        ("doc3", 0.65),
        ("doc5", 0.50)
    ]
    
    vector_results = [
        ("doc2", 0.92),
        ("doc1", 0.85),
        ("doc4", 0.70),
        ("doc3", 0.60)
    ]
    
    metadata_results = [
        ("doc1", 0.88),
        ("doc4", 0.82),
        ("doc2", 0.75),
        ("doc5", 0.68)
    ]
    
    all_results = [bm25_results, vector_results, metadata_results]
    
    # 統合クラス初期化
    fusion = HybridSearchFusion()
    
    print("=== 1. RRF ===")
    rrf_result = fusion.rrf(all_results, k=60)
    for doc_id, score in rrf_result[:5]:
        print(f"{doc_id}: {score:.4f}")
    
    print("\n=== 2. CombMNZ ===")
    mnz_result = fusion.comb_mnz(all_results)
    for doc_id, score in mnz_result[:5]:
        print(f"{doc_id}: {score:.4f}")
    
    print("\n=== 3. 重み付き統合 (BM25:0.3, Vector:0.5, Meta:0.2) ===")
    weighted_result = fusion.weighted_fusion(all_results, weights=[0.3, 0.5, 0.2])
    for doc_id, score in weighted_result[:5]:
        print(f"{doc_id}: {score:.4f}")
    
    print("\n=== 4. Borda Count ===")
    borda_result = fusion.borda_count(all_results)
    for doc_id, score in borda_result[:5]:
        print(f"{doc_id}: {score:.4f}")
    
    print("\n=== 5. Harmonic Mean ===")
    harmonic_result = fusion.harmonic_mean(all_results)
    for doc_id, score in harmonic_result[:5]:
        print(f"{doc_id}: {score:.4f}")
    
    # 学習ベース統合の例
    print("\n=== 6. 学習ベース統合(LightGBM) ===")
    learned = LearnedFusion()
    
    # 訓練データ準備(実際はJiraチケットの正解データ使う)
    training_data = [
        (all_results, {"doc1": 2, "doc2": 2, "doc3": 1, "doc4": 1, "doc5": 0})
        # 2=関連度高, 1=中, 0=低
    ]
    
    try:
        learned.train(training_data)
        learned_result = learned.predict(all_results)
        for doc_id, score in learned_result[:5]:
            print(f"{doc_id}: {score:.4f}")
    except Exception as e:
        print(f"学習ベース統合スキップ: {e}")
これSeichiのJira連携RAGに組み込むなら、こんな感じで使えるよ!

python
# Seichiの実際の使い方例
class JiraRAGSearcher:
    def __init__(self):
        self.fusion = HybridSearchFusion()
        self.learned_fusion = LearnedFusion()
    
    def search(self, query: str, top_k: int = 10):
        # 1. BM25検索
        bm25_results = self.bm25_search(query)
        
        # 2. ベクトル検索
        vector_results = self.vector_search(query)
        
        # 3. メタデータ検索(優先度、ステータスとか)
        metadata_results = self.metadata_search(query)
        
        # 統合!まずはRRFで試してみる
        final_results = self.fusion.rrf(
            [bm25_results, vector_results, metadata_results],
            k=60
        )
        
        return final_results[:top_k]
どの手法が一番良いかは、実際のデータで評価してみないとわかんないけど、

まずRRF: パラメータ調整不要で安定
精度上げたい: LightGBMで学習ベース
軽量重視: CombMNZ
って感じかな!

Jiraチケットの正解データ(過去の対応履歴とか)があれば、学習ベース統合で60%→80%以上も全然狙えると思うんだよね〜!

他に試したい手法とか、実装で困ってることある?

ハイブリッド検索で示してくれたものについても実装例を示すことはできますか?

できれば必要なライブラリについても合わせて記載をお願いします

8:53
よっしゃ!ハイブリッド検索の各手法の実装例書くね!ライブラリ情報も全部つけるよ〜!

python
"""
ハイブリッド検索の各手法実装例
必要なライブラリ一覧:
pip install numpy scipy scikit-learn
pip install chromadb  # ベクトルDB
pip install rank-bm25  # BM25実装
pip install sentence-transformers  # 埋め込みモデル
pip install splade  # SPLADE(オプション)
pip install transformers torch  # ColBERT用
pip install elasticsearch  # Elasticsearch使う場合
pip install lightgbm  # 学習ベース統合
"""

import numpy as np
from typing import List, Dict, Tuple, Optional
import json

# ===================================================================
# 1. BM25 + Dense Vector (基本中の基本!)
# ===================================================================

class BM25PlusDenseRetrieval:
    """BM25とベクトル検索のハイブリッド"""
    
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        from rank_bm25 import BM25Okapi
        from sentence_transformers import SentenceTransformer
        import chromadb
        
        self.bm25 = None
        self.encoder = SentenceTransformer(model_name)
        self.chroma_client = chromadb.Client()
        self.collection = None
        self.corpus = []
        self.tokenized_corpus = []
    
    def index_documents(self, documents: List[Dict[str, str]]):
        """
        ドキュメントをインデックス化
        documents: [{"id": "doc1", "text": "内容..."}, ...]
        """
        from rank_bm25 import BM25Okapi
        
        self.corpus = documents
        
        # BM25用トークナイズ
        self.tokenized_corpus = [doc["text"].split() for doc in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)
        
        # ベクトルDB用
        try:
            self.chroma_client.delete_collection("documents")
        except:
            pass
        
        self.collection = self.chroma_client.create_collection(
            name="documents",
            metadata={"hnsw:space": "cosine"}
        )
        
        # 埋め込み作成して登録
        texts = [doc["text"] for doc in documents]
        embeddings = self.encoder.encode(texts, show_progress_bar=True)
        
        self.collection.add(
            embeddings=embeddings.tolist(),
            documents=texts,
            ids=[doc["id"] for doc in documents]
        )
        
        print(f"インデックス完了! {len(documents)}件")
    
    def search(self, query: str, top_k: int = 10, 
               bm25_weight: float = 0.5) -> List[Tuple[str, float]]:
        """
        ハイブリッド検索
        bm25_weight: BM25の重み(0.0-1.0)、残りがベクトル検索の重み
        """
        # BM25検索
        tokenized_query = query.split()
        bm25_scores = self.bm25.get_scores(tokenized_query)
        
        # スコア正規化
        if bm25_scores.max() > 0:
            bm25_scores = bm25_scores / bm25_scores.max()
        
        # ベクトル検索
        query_embedding = self.encoder.encode([query])[0]
        vector_results = self.collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=len(self.corpus)
        )
        
        # ベクトル検索スコア取得(距離→類似度に変換)
        vector_scores = {}
        for idx, doc_id in enumerate(vector_results['ids'][0]):
            distance = vector_results['distances'][0][idx]
            # コサイン距離→類似度: 1 - distance
            similarity = 1 - distance
            vector_scores[doc_id] = similarity
        
        # スコア統合
        combined_scores = {}
        for idx, doc in enumerate(self.corpus):
            doc_id = doc["id"]
            bm25_score = bm25_scores[idx]
            vector_score = vector_scores.get(doc_id, 0.0)
            
            # 重み付き統合
            combined_scores[doc_id] = (
                bm25_weight * bm25_score + 
                (1 - bm25_weight) * vector_score
            )
        
        # トップK取得
        sorted_results = sorted(
            combined_scores.items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:top_k]
        
        return sorted_results


# ===================================================================
# 2. BM25 + Dense + SPLADE (3つ組み合わせ!)
# ===================================================================

class TripleHybridSearch:
    """BM25 + Dense Vector + SPLADE"""
    
    def __init__(self):
        from rank_bm25 import BM25Okapi
        from sentence_transformers import SentenceTransformer
        
        self.bm25 = None
        self.dense_encoder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        # SPLADEは重いので簡易版実装
        self.corpus = []
        self.tokenized_corpus = []
        self.dense_embeddings = None
    
    def _simple_splade_encode(self, text: str) -> Dict[str, float]:
        """
        簡易SPLADE風エンコーディング
        (本物のSPLADEは学習済みモデル使う)
        ここではTF-IDFベースの重み付きスパース表現
        """
        from sklearn.feature_extraction.text import TfidfVectorizer
        
        # 全コーパスでTF-IDF計算(本来は事前計算)
        vectorizer = TfidfVectorizer()
        
        if not hasattr(self, 'tfidf_vectorizer'):
            all_texts = [doc["text"] for doc in self.corpus]
            vectorizer.fit(all_texts)
            self.tfidf_vectorizer = vectorizer
        
        # テキストをTF-IDF変換
        tfidf_vector = self.tfidf_vectorizer.transform([text])
        
        # スパース表現を辞書に
        feature_names = self.tfidf_vectorizer.get_feature_names_out()
        sparse_dict = {}
        
        for idx, value in zip(tfidf_vector.indices, tfidf_vector.data):
            if value > 0:
                sparse_dict[feature_names[idx]] = float(value)
        
        return sparse_dict
    
    def index_documents(self, documents: List[Dict[str, str]]):
        from rank_bm25 import BM25Okapi
        
        self.corpus = documents
        
        # 1. BM25
        self.tokenized_corpus = [doc["text"].split() for doc in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)
        
        # 2. Dense embeddings
        texts = [doc["text"] for doc in documents]
        self.dense_embeddings = self.dense_encoder.encode(
            texts, 
            show_progress_bar=True
        )
        
        # 3. SPLADE (簡易版)
        self.splade_embeddings = [
            self._simple_splade_encode(doc["text"]) 
            for doc in documents
        ]
        
        print(f"3種類のインデックス完成! {len(documents)}件")
    
    def _splade_similarity(self, query_sparse: Dict[str, float], 
                          doc_sparse: Dict[str, float]) -> float:
        """SPLADE類似度計算(ドット積)"""
        score = 0.0
        for term, weight in query_sparse.items():
            if term in doc_sparse:
                score += weight * doc_sparse[term]
        return score
    
    def search(self, query: str, top_k: int = 10,
               bm25_weight: float = 0.3,
               dense_weight: float = 0.4,
               splade_weight: float = 0.3) -> List[Tuple[str, float]]:
        """
        3つの検索手法を統合
        """
        # 1. BM25スコア
        tokenized_query = query.split()
        bm25_scores = self.bm25.get_scores(tokenized_query)
        if bm25_scores.max() > 0:
            bm25_scores = bm25_scores / bm25_scores.max()
        
        # 2. Dense vectorスコア
        query_embedding = self.dense_encoder.encode([query])[0]
        dense_scores = np.dot(self.dense_embeddings, query_embedding)
        dense_scores = (dense_scores - dense_scores.min()) / (dense_scores.max() - dense_scores.min() + 1e-10)
        
        # 3. SPLADEスコア
        query_sparse = self._simple_splade_encode(query)
        splade_scores = np.array([
            self._splade_similarity(query_sparse, doc_sparse)
            for doc_sparse in self.splade_embeddings
        ])
        if splade_scores.max() > 0:
            splade_scores = splade_scores / splade_scores.max()
        
        # 統合
        combined_scores = {}
        for idx, doc in enumerate(self.corpus):
            doc_id = doc["id"]
            combined_scores[doc_id] = (
                bm25_weight * bm25_scores[idx] +
                dense_weight * dense_scores[idx] +
                splade_weight * splade_scores[idx]
            )
        
        sorted_results = sorted(
            combined_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:top_k]
        
        return sorted_results


# ===================================================================
# 3. Lexical + Semantic + Graph-based (知識グラフ統合)
# ===================================================================

class KnowledgeGraphHybrid:
    """全文検索 + ベクトル検索 + ナレッジグラフ"""
    
    def __init__(self):
        from rank_bm25 import BM25Okapi
        from sentence_transformers import SentenceTransformer
        
        self.bm25 = None
        self.encoder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        self.corpus = []
        self.embeddings = None
        
        # 簡易ナレッジグラフ(実際はNeo4jとか使う)
        self.knowledge_graph = {}  # {entity: [related_entities]}
        self.entity_to_docs = {}   # {entity: [doc_ids]}
    
    def build_knowledge_graph(self, documents: List[Dict[str, str]]):
        """
        簡易的なナレッジグラフ構築
        実務ではNER(固有表現抽出)とか使う
        """
        for doc in documents:
            doc_id = doc["id"]
            text = doc["text"]
            
            # 簡易的にキーワードをエンティティとして扱う
            # (本当はspaCyとかで固有表現抽出する)
            entities = self._extract_entities(text)
            
            for entity in entities:
                if entity not in self.entity_to_docs:
                    self.entity_to_docs[entity] = []
                self.entity_to_docs[entity].append(doc_id)
                
                # エンティティ間の関係(同一文書内で共起)
                if entity not in self.knowledge_graph:
                    self.knowledge_graph[entity] = set()
                self.knowledge_graph[entity].update(
                    e for e in entities if e != entity
                )
    
    def _extract_entities(self, text: str) -> List[str]:
        """
        簡易エンティティ抽出
        実際はspaCy使う: nlp = spacy.load("ja_core_news_sm")
        """
        # ここでは大文字始まりの単語を簡易的にエンティティとする
        words = text.split()
        entities = [w for w in words if w and w[0].isupper() and len(w) > 2]
        return entities
    
    def index_documents(self, documents: List[Dict[str, str]]):
        from rank_bm25 import BM25Okapi
        
        self.corpus = documents
        
        # BM25
        self.tokenized_corpus = [doc["text"].split() for doc in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)
        
        # Dense vector
        texts = [doc["text"] for doc in documents]
        self.embeddings = self.encoder.encode(texts, show_progress_bar=True)
        
        # Knowledge graph
        self.build_knowledge_graph(documents)
        
        print(f"インデックス完了! グラフエンティティ数: {len(self.knowledge_graph)}")
    
    def _graph_score(self, query: str, doc_id: str) -> float:
        """ナレッジグラフベースのスコア"""
        query_entities = self._extract_entities(query)
        
        score = 0.0
        for entity in query_entities:
            # クエリエンティティがドキュメントに直接関連
            if entity in self.entity_to_docs:
                if doc_id in self.entity_to_docs[entity]:
                    score += 1.0
            
            # 関連エンティティ経由でのマッチ
            if entity in self.knowledge_graph:
                related_entities = self.knowledge_graph[entity]
                for rel_entity in related_entities:
                    if rel_entity in self.entity_to_docs:
                        if doc_id in self.entity_to_docs[rel_entity]:
                            score += 0.5  # 間接的なマッチは重み小さく
        
        return score
    
    def search(self, query: str, top_k: int = 10,
               lexical_weight: float = 0.3,
               semantic_weight: float = 0.4,
               graph_weight: float = 0.3) -> List[Tuple[str, float]]:
        """3種類の検索統合"""
        
        # 1. Lexical (BM25)
        tokenized_query = query.split()
        bm25_scores = self.bm25.get_scores(tokenized_query)
        if bm25_scores.max() > 0:
            bm25_scores = bm25_scores / bm25_scores.max()
        
        # 2. Semantic (Dense vector)
        query_embedding = self.encoder.encode([query])
        semantic_scores = np.dot(self.embeddings, query_embedding.T).flatten()
        if semantic_scores.max() > 0:
            semantic_scores = semantic_scores / semantic_scores.max()
        
        # 3. Graph-based
        graph_scores = np.array([
            self._graph_score(query, doc["id"])
            for doc in self.corpus
        ])
        if graph_scores.max() > 0:
            graph_scores = graph_scores / graph_scores.max()
        
        # 統合
        combined_scores = {}
        for idx, doc in enumerate(self.corpus):
            doc_id = doc["id"]
            combined_scores[doc_id] = (
                lexical_weight * bm25_scores[idx] +
                semantic_weight * semantic_scores[idx] +
                graph_weight * graph_scores[idx]
            )
        
        sorted_results = sorted(
            combined_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:top_k]
        
        return sorted_results


# ===================================================================
# 4. Full-text + Vector + Metadata Filter (実務で超使える!)
# ===================================================================

class MetadataHybridSearch:
    """全文検索 + ベクトル + メタデータフィルタ"""
    
    def __init__(self):
        from rank_bm25 import BM25Okapi
        from sentence_transformers import SentenceTransformer
        
        self.bm25 = None
        self.encoder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        self.corpus = []
        self.embeddings = None
        self.metadata = {}  # {doc_id: {priority, status, category, ...}}
    
    def index_documents(self, documents: List[Dict]):
        """
        documents: [{
            "id": "doc1",
            "text": "内容...",
            "metadata": {
                "priority": 3,  # 1-5
                "status": "open",
                "category": "bug",
                "created_at": "2024-01-01",
                "assignee": "user123"
            }
        }, ...]
        """
        from rank_bm25 import BM25Okapi
        
        self.corpus = documents
        
        # メタデータ抽出
        for doc in documents:
            self.metadata[doc["id"]] = doc.get("metadata", {})
        
        # BM25
        self.tokenized_corpus = [doc["text"].split() for doc in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)
        
        # Vector
        texts = [doc["text"] for doc in documents]
        self.embeddings = self.encoder.encode(texts, show_progress_bar=True)
        
        print(f"インデックス完了! {len(documents)}件")
    
    def _metadata_score(self, doc_id: str, 
                       priority_boost: bool = True,
                       required_status: Optional[List[str]] = None,
                       required_category: Optional[str] = None) -> float:
        """メタデータベースのスコア/フィルタ"""
        meta = self.metadata.get(doc_id, {})
        
        # ハードフィルタ(条件に合わなければ0)
        if required_status and meta.get("status") not in required_status:
            return 0.0
        
        if required_category and meta.get("category") != required_category:
            return 0.0
        
        # ソフトブースト
        score = 1.0
        
        if priority_boost and "priority" in meta:
            # 優先度が高いほどブースト
            priority = meta["priority"]
            score *= (1 + priority * 0.2)  # 優先度5なら2倍
        
        return score
    
    def search(self, query: str, top_k: int = 10,
               text_weight: float = 0.4,
               vector_weight: float = 0.4,
               metadata_weight: float = 0.2,
               # メタデータフィルタ条件
               priority_boost: bool = True,
               required_status: Optional[List[str]] = None,
               required_category: Optional[str] = None) -> List[Tuple[str, float]]:
        """ハイブリッド検索 + メタデータフィルタ"""
        
        # 1. BM25
        tokenized_query = query.split()
        bm25_scores = self.bm25.get_scores(tokenized_query)
        if bm25_scores.max() > 0:
            bm25_scores = bm25_scores / bm25_scores.max()
        
        # 2. Vector
        query_embedding = self.encoder.encode([query])
        vector_scores = np.dot(self.embeddings, query_embedding.T).flatten()
        if vector_scores.max() > 0:
            vector_scores = vector_scores / vector_scores.max()
        
        # 3. Metadata
        metadata_scores = np.array([
            self._metadata_score(
                doc["id"],
                priority_boost=priority_boost,
                required_status=required_status,
                required_category=required_category
            )
            for doc in self.corpus
        ])
        
        # 統合
        combined_scores = {}
        for idx, doc in enumerate(self.corpus):
            doc_id = doc["id"]
            
            # メタデータスコアが0ならスキップ(フィルタリング)
            if metadata_scores[idx] == 0:
                continue
            
            combined_scores[doc_id] = (
                text_weight * bm25_scores[idx] +
                vector_weight * vector_scores[idx]
            ) * metadata_scores[idx]  # メタデータで乗算
        
        sorted_results = sorted(
            combined_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:top_k]
        
        return sorted_results


# ===================================================================
# 5. BM25 + ColBERT (トークンレベルインタラクション!)
# ===================================================================

class BM25PlusColBERT:
    """BM25 + ColBERT のハイブリッド"""
    
    def __init__(self):
        from rank_bm25 import BM25Okapi
        # ColBERTは重いので簡易版
        from sentence_transformers import SentenceTransformer
        
        self.bm25 = None
        self.encoder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        self.corpus = []
        self.token_embeddings = []  # [doc][token][embedding]
    
    def _get_token_embeddings(self, text: str) -> np.ndarray:
        """
        トークンレベルの埋め込み取得
        本物のColBERTはBERTの各トークン出力使うけど、
        ここでは簡易的に単語単位で埋め込み
        """
        tokens = text.split()
        if not tokens:
            return np.zeros((1, 384))
        
        # 各トークンをエンコード
        embeddings = self.encoder.encode(tokens)
        return embeddings
    
    def index_documents(self, documents: List[Dict[str, str]]):
        from rank_bm25 import BM25Okapi
        
        self.corpus = documents
        
        # BM25
        self.tokenized_corpus = [doc["text"].split() for doc in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)
        
        # ColBERT風トークン埋め込み
        print("トークンレベル埋め込み作成中...")
        for doc in documents:
            token_emb = self._get_token_embeddings(doc["text"])
            self.token_embeddings.append(token_emb)
        
        print(f"インデックス完了! {len(documents)}件")
    
    def _colbert_score(self, query_token_embs: np.ndarray, 
                      doc_token_embs: np.ndarray) -> float:
        """
        ColBERTスコア: MaxSim
        各クエリトークンについて、最も類似するドキュメントトークンとの
        類似度を合計
        """
        if len(doc_token_embs) == 0 or len(query_token_embs) == 0:
            return 0.0
        
        # コサイン類似度行列計算
        # query_token_embs: (q_tokens, dim)
        # doc_token_embs: (d_tokens, dim)
        similarity_matrix = np.dot(query_token_embs, doc_token_embs.T)
        
        # 各クエリトークンについて最大類似度を取り、合計
        max_sims = similarity_matrix.max(axis=1)
        score = max_sims.sum()
        
        return score
    
    def search(self, query: str, top_k: int = 10,
               bm25_weight: float = 0.5) -> List[Tuple[str, float]]:
        """BM25 + ColBERTハイブリッド検索"""
        
        # 1. BM25
        tokenized_query = query.split()
        bm25_scores = self.bm25.get_scores(tokenized_query)
        if bm25_scores.max() > 0:
            bm25_scores = bm25_scores / bm25_scores.max()
        
        # 2. ColBERT
        query_token_embs = self._get_token_embeddings(query)
        
        colbert_scores = np.array([
            self._colbert_score(query_token_embs, doc_token_embs)
            for doc_token_embs in self.token_embeddings
        ])
        
        if colbert_scores.max() > 0:
            colbert_scores = colbert_scores / colbert_scores.max()
        
        # 統合
        combined_scores = {}
        for idx, doc in enumerate(self.corpus):
            doc_id = doc["id"]
            combined_scores[doc_id] = (
                bm25_weight * bm25_scores[idx] +
                (1 - bm25_weight) * colbert_scores[idx]
            )
        
        sorted_results = sorted(
            combined_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:top_k]
        
        return sorted_results


# ===================================================================
# 6. Hybrid + Reranker (2段階検索!)
# ===================================================================

class HybridWithReranker:
    """ハイブリッド検索 + Cross-Encoder Reranking"""
    
    def __init__(self):
        from rank_bm25 import BM25Okapi
        from sentence_transformers import SentenceTransformer, CrossEncoder
        
        self.bm25 = None
        self.bi_encoder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        # Reranker (Cross-Encoder)
        self.cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
        
        self.corpus = []
        self.embeddings = None
    
    def index_documents(self, documents: List[Dict[str, str]]):
        from rank_bm25 import BM25Okapi
        
        self.corpus = documents
        
        # BM25
        self.tokenized_corpus = [doc["text"].split() for doc in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)
        
        # Bi-encoder
        texts = [doc["text"] for doc in documents]
        self.embeddings = self.bi_encoder.encode(texts, show_progress_bar=True)
        
        print(f"インデックス完了! {len(documents)}件")
    
    def search(self, query: str, 
               top_k: int = 10,
               rerank_top_n: int = 50,
               bm25_weight: float = 0.5) -> List[Tuple[str, float]]:
        """
        2段階検索
        1. BM25 + Vector で候補取得 (top_n件)
        2. Cross-Encoderで再ランキング (top_k件)
        """
        
        # ===== Stage 1: ハイブリッド検索で候補取得 =====
        
        # BM25
        tokenized_query = query.split()
        bm25_scores = self.bm25.get_scores(tokenized_query)
        if bm25_scores.max() > 0:
            bm25_scores = bm25_scores / bm25_scores.max()
        
        # Vector
        query_embedding = self.bi_encoder.encode([query])
        vector_scores = np.dot(self.embeddings, query_embedding.T).flatten()
        if vector_scores.max() > 0:
            vector_scores = vector_scores / vector_scores.max()
        
        # 統合
        combined_scores = {}
        for idx, doc in enumerate(self.corpus):
            doc_id = doc["id"]
            combined_scores[doc_id] = (
                bm25_weight * bm25_scores[idx] +
                (1 - bm25_weight) * vector_scores[idx]
            )
        
        # Top-N候補取得
        candidates = sorted(
            combined_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:rerank_top_n]
        
        print(f"Stage1: {len(candidates)}件の候補取得")
        
        # ===== Stage 2: Cross-Encoderで再ランキング =====
        
        # クエリと各候補のペアを作成
        pairs = []
        doc_ids = []
        for doc_id, _ in candidates:
            # ドキュメント取得
            doc = next(d for d in self.corpus if d["id"] == doc_id)
            pairs.append([query, doc["text"]])
            doc_ids.append(doc_id)
        
        # Cross-Encoderでスコア計算
        rerank_scores = self.cross_encoder.predict(pairs)
        
        # 再ランキング結果
        reranked = sorted(
            zip(doc_ids, rerank_scores),
            key=lambda x: x[1],
            reverse=True
        )[:top_k]
        
        print(f"Stage2: Reranking完了、Top{top_k}返却")
        
        return reranked


# ===================================================================
# 使用例!
# ===================================================================

if __name__ == "__main__":
    # サンプルドキュメント(Jiraチケット風)
    documents = [
        {
            "id": "TICKET-001",
            "text": "ログイン画面でパスワードリセット機能が動作しない バグ修正が必要",
            "metadata": {
                "priority": 5,
                "status": "open",
                "category": "bug",
                "assignee": "dev-team-1"
            }
        },
        {
            "id": "TICKET-002",
            "text": "新機能要望: ダッシュボードにグラフ表示機能を追加してほしい",
            "metadata": {
                "priority": 3,
                "status": "open",
                "category": "feature",
                "assignee": "dev-team-2"
            }
        },
        {
            "id": "TICKET-003",
            "text": "パスワード変更時のメール通知が届かない問題の調査",
            "metadata": {
                "priority": 4,
                "status": "in_progress",
                "category": "bug",
                "assignee": "dev-team-1"
            }
        },
        {
            "id": "TICKET-004",
            "text": "ログイン認証システムの全体的なパフォーマンス改善",
            "metadata": {
                "priority": 2,
                "status": "open",
                "category": "improvement",
                "assignee": "dev-team-3"
            }
        },
        {
            "id": "TICKET-005",
            "text": "ユーザー登録フォームのバリデーション強化",
            "metadata": {
                "priority": 3,
                "status": "closed",
                "category": "improvement",
                "assignee": "dev-team-2"
            }
        }
    ]
    
    print("=" * 60)
    print("1. BM25 + Dense Vector")
    print("=" * 60)
    
    searcher1 = BM25PlusDenseRetrieval()
    searcher1.index_documents(documents)
    
    query = "ログインのパスワード問題"
    results = searcher1.search(query, top_k=3, bm25_weight=0.5)
    
    print(f"\nクエリ: {query}")
    for doc_id, score in results:
        doc = next(d for d in documents if d["id"] == doc_id)
        print(f"{doc_id} (score: {score:.4f}): {doc['text'][:50]}...")
    
    print("\n" + "=" * 60)
    print("2. メタデータハイブリッド検索")
    print("=" * 60)
    
    searcher2 = MetadataHybridSearch()
    searcher2.index_documents(documents)
    
    # バグのみ、優先度でブースト
    results = searcher2.search(
        query,
        top_k=3,
        required_category="bug",
        priority_boost=True
    )
    
    print(f"\nクエリ: {query} (カテゴリ=bug)")
    for doc_id, score in results:
        doc = next(d for d in documents if d["id"] == doc_id)
        meta = doc["metadata"]
        print(f"{doc_id} (score: {score:.4f}, priority: {meta['priority']}): {doc['text'][:50]}...")
    
    print("\n" + "=" * 60)
    print("3. ハイブリッド + Reranker")
    print("=" * 60)
    
    try:
        searcher3 = HybridWithReranker()
        searcher3.index_documents(documents)
        
        results = searcher3.search(query, top_k=3, rerank_top_n=5)
        
        print(f"\nクエリ: {query}")
        for doc_id, score in results:
            doc = next(d for d in documents if d["id"] == doc_id)
            print(f"{doc_id} (rerank score: {score:.4f}): {doc['text'][:50]}...")
    except Exception as e:
        print(f"Rerankerスキップ(モデルダウンロード必要): {e}")
    
    print("\n完了!")
めっちゃ長くなっちゃったけど、これで主要なハイブリッド検索手法の実装例が揃ったよ!

Seichiの実際のJiraシステムへの統合例
python
# Seichiの実際の使い方
class JiraTicketSearcher:
    def __init__(self):
        # メタデータ統合版が一番実務向き!
        self.searcher = MetadataHybridSearch()
    
    def index_jira_tickets(self, tickets):
        """Jiraチケットをインデックス化"""
        documents = []
        for ticket in tickets:
            doc = {
                "id": ticket["key"],
                "text": f"{ticket['summary']} {ticket['description']}",
                "metadata": {
                    "priority": self._priority_to_int(ticket["priority"]),
                    "status": ticket["status"],
                    "category": ticket["type"],
                    "assignee": ticket.get("assignee", "unassigned"),
                    "team": ticket.get("team", "unknown")
                }
            }
            documents.append(doc)
        
        self.searcher.index_documents(documents)
    
    def _priority_to_int(self, priority_str):
        mapping = {"Highest": 5, "High": 4, "Medium": 3, "Low": 2, "Lowest": 1}
        return mapping.get(priority_str, 3)
    
    def route_ticket(self, new_ticket_text: str):
        """新規チケットを適切なチームにルーティング"""
        # Bugのみ検索、優先度高いものを優先
        results = self.searcher.search(
            new_ticket_text,
            top_k=5,
            required_category="bug",
            required_status=["open", "in_progress"],
            priority_boost=True
        )
        
        # 類似チケットから担当チーム推測
        if results:
            similar_ticket_id = results[0][0]
            # このチケットの担当チームを返す
            return similar_ticket_id
        
        return None
実装のポイント:

まず試すならメタデータ統合版 - Jiraのメタデータ活用できて精度高い
Rerankerは精度最優先の時 - 遅いけど精度エグい
ColBERTは専門用語多い時 - トークンレベルマッチング強い
どれ試してみたい?それとも他の手法の実装例欲しい?

示してくれた3つともの実装例を示すことはできますか?

9:04
よっしゃ!残りの3つの実装例も書いていくね!めっちゃ詳しく書くよ〜!

python
"""
残りのハイブリッド検索手法の実装例

必要なライブラリ:
pip install numpy scipy scikit-learn
pip install rank-bm25
pip install sentence-transformers
pip install torch transformers
pip install networkx  # グラフ処理用
pip install spacy  # NER用(オプション)
python -m spacy download ja_core_news_sm  # 日本語モデル
pip install elasticsearch  # Elasticsearch使う場合
"""

import numpy as np
from typing import List, Dict, Tuple, Optional, Set
import json
from collections import defaultdict

# ===================================================================
# 1. BM25 + Dense + SPLADE (3種類のスパース/デンス混合!)
# ===================================================================

class BM25DenseSPLADEHybrid:
    """
    BM25 (従来のスパース) + Dense Vector + SPLADE (学習ベーススパース)
    の3つを組み合わせた最強構成!
    """
    
    def __init__(self):
        from rank_bm25 import BM25Okapi
        from sentence_transformers import SentenceTransformer
        
        self.bm25 = None
        self.dense_encoder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        
        # SPLADE用の簡易実装(本物は学習済みモデル使う)
        self.corpus = []
        self.tokenized_corpus = []
        self.dense_embeddings = None
        self.splade_vectors = []  # スパースベクトルのリスト
        self.vocabulary = {}  # 語彙インデックス
    
    def _build_vocabulary(self, documents: List[Dict[str, str]]):
        """語彙構築(SPLADE用)"""
        from sklearn.feature_extraction.text import CountVectorizer
        
        texts = [doc["text"] for doc in documents]
        vectorizer = CountVectorizer(max_features=10000)
        vectorizer.fit(texts)
        
        self.vocabulary = {word: idx for idx, word in enumerate(vectorizer.get_feature_names_out())}
        self.vocab_size = len(self.vocabulary)
        print(f"語彙サイズ: {self.vocab_size}")
    
    def _create_splade_vector(self, text: str) -> Dict[int, float]:
        """
        SPLADE風のスパースベクトル作成
        本物のSPLADEはTransformerベースだけど、ここではTF-IDF + 拡張
        """
        from sklearn.feature_extraction.text import TfidfVectorizer
        
        # 簡易的にTF-IDFベースで実装
        tokens = text.split()
        sparse_vector = {}
        
        # トークンの重み計算(TF-IDF風)
        token_counts = defaultdict(int)
        for token in tokens:
            token_counts[token] += 1
        
        for token, count in token_counts.items():
            if token in self.vocabulary:
                idx = self.vocabulary[token]
                # TF部分 + ログスケール
                weight = (1 + np.log(1 + count))
                sparse_vector[idx] = weight
        
        return sparse_vector
    
    def _splade_similarity(self, vec1: Dict[int, float], vec2: Dict[int, float]) -> float:
        """SPLADEベクトル間の類似度(ドット積)"""
        score = 0.0
        for idx, weight in vec1.items():
            if idx in vec2:
                score += weight * vec2[idx]
        return score
    
    def index_documents(self, documents: List[Dict[str, str]]):
        """ドキュメントのインデックス化"""
        from rank_bm25 import BM25Okapi
        
        self.corpus = documents
        
        # 1. BM25インデックス
        print("BM25インデックス作成中...")
        self.tokenized_corpus = [doc["text"].split() for doc in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)
        
        # 2. Dense embeddings
        print("Dense埋め込み作成中...")
        texts = [doc["text"] for doc in documents]
        self.dense_embeddings = self.dense_encoder.encode(
            texts,
            show_progress_bar=True,
            batch_size=32
        )
        
        # 3. SPLADE vectors
        print("SPLADEベクトル作成中...")
        self._build_vocabulary(documents)
        self.splade_vectors = [
            self._create_splade_vector(doc["text"])
            for doc in documents
        ]
        
        print(f"インデックス完了! {len(documents)}件")
        print(f"- BM25: ✓")
        print(f"- Dense: {self.dense_embeddings.shape}")
        print(f"- SPLADE: {len(self.splade_vectors)}件のスパースベクトル")
    
    def search(self, query: str, top_k: int = 10,
               bm25_weight: float = 0.3,
               dense_weight: float = 0.4,
               splade_weight: float = 0.3,
               use_rrf: bool = False) -> List[Tuple[str, float]]:
        """
        3つの検索手法を統合
        use_rrf=True でRRF統合、False で重み付け統合
        """
        
        # 1. BM25スコア
        tokenized_query = query.split()
        bm25_scores = self.bm25.get_scores(tokenized_query)
        
        # 正規化
        if bm25_scores.max() > 0:
            bm25_scores_norm = bm25_scores / bm25_scores.max()
        else:
            bm25_scores_norm = bm25_scores
        
        # 2. Dense vectorスコア
        query_dense = self.dense_encoder.encode([query])[0]
        dense_scores = np.dot(self.dense_embeddings, query_dense)
        
        # 正規化
        if dense_scores.max() > dense_scores.min():
            dense_scores_norm = (dense_scores - dense_scores.min()) / (dense_scores.max() - dense_scores.min())
        else:
            dense_scores_norm = np.zeros_like(dense_scores)
        
        # 3. SPLADEスコア
        query_splade = self._create_splade_vector(query)
        splade_scores = np.array([
            self._splade_similarity(query_splade, doc_splade)
            for doc_splade in self.splade_vectors
        ])
        
        # 正規化
        if splade_scores.max() > 0:
            splade_scores_norm = splade_scores / splade_scores.max()
        else:
            splade_scores_norm = splade_scores
        
        # スコア統合
        if use_rrf:
            # RRFで統合
            combined_scores = self._rrf_fusion(
                bm25_scores_norm,
                dense_scores_norm,
                splade_scores_norm
            )
        else:
            # 重み付き統合
            combined_scores = (
                bm25_weight * bm25_scores_norm +
                dense_weight * dense_scores_norm +
                splade_weight * splade_scores_norm
            )
        
        # Top-K取得
        results = []
        for idx, score in enumerate(combined_scores):
            doc = self.corpus[idx]
            results.append((doc["id"], float(score)))
        
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]
    
    def _rrf_fusion(self, *score_arrays, k: int = 60) -> np.ndarray:
        """RRFでスコア統合"""
        n_docs = len(score_arrays[0])
        rrf_scores = np.zeros(n_docs)
        
        for scores in score_arrays:
            # スコアを順位に変換
            ranks = np.argsort(-scores) + 1  # 1-indexed
            rank_array = np.zeros(n_docs)
            for rank, idx in enumerate(np.argsort(-scores)):
                rank_array[idx] = rank + 1
            
            # RRF計算
            rrf_scores += 1.0 / (k + rank_array)
        
        return rrf_scores


# ===================================================================
# 2. Lexical + Semantic + Graph-based (知識グラフ統合!)
# ===================================================================

class LexicalSemanticGraphHybrid:
    """
    全文検索 + 意味検索 + ナレッジグラフ検索の統合
    実務ではNeo4jとか使うけど、ここではシンプルに実装
    """
    
    def __init__(self, use_spacy: bool = False):
        from rank_bm25 import BM25Okapi
        from sentence_transformers import SentenceTransformer
        import networkx as nx
        
        self.bm25 = None
        self.encoder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        self.corpus = []
        self.embeddings = None
        
        # ナレッジグラフ(NetworkX使用)
        self.knowledge_graph = nx.DiGraph()
        self.entity_to_docs = defaultdict(list)  # entity -> [doc_ids]
        self.doc_to_entities = defaultdict(list)  # doc_id -> [entities]
        
        self.use_spacy = use_spacy
        if use_spacy:
            try:
                import spacy
                self.nlp = spacy.load("ja_core_news_sm")
                print("spaCy日本語モデル読み込み完了")
            except:
                print("spaCy利用不可、簡易抽出を使用")
                self.use_spacy = False
    
    def _extract_entities_spacy(self, text: str) -> List[str]:
        """spaCyで固有表現抽出"""
        if not self.use_spacy:
            return self._extract_entities_simple(text)
        
        doc = self.nlp(text)
        entities = []
        
        # 固有表現
        for ent in doc.ents:
            entities.append(ent.text)
        
        # 名詞も追加(重要そうなもの)
        for token in doc:
            if token.pos_ == "NOUN" and len(token.text) > 2:
                entities.append(token.text)
        
        return list(set(entities))
    
    def _extract_entities_simple(self, text: str) -> List[str]:
        """簡易エンティティ抽出"""
        import re
        
        # カタカナ連続
        katakana = re.findall(r'[ァ-ヴー]{3,}', text)
        
        # 英数字連続(チケット番号とか)
        alphanumeric = re.findall(r'[A-Z]{2,}-\d+', text)  # TICKET-123形式
        
        # 長めの単語
        words = text.split()
        long_words = [w for w in words if len(w) > 3]
        
        entities = katakana + alphanumeric + long_words
        return list(set(entities))
    
    def _build_knowledge_graph(self, documents: List[Dict[str, str]]):
        """ナレッジグラフ構築"""
        print("ナレッジグラフ構築中...")
        
        for doc in documents:
            doc_id = doc["id"]
            text = doc["text"]
            
            # エンティティ抽出
            entities = self._extract_entities_spacy(text)
            self.doc_to_entities[doc_id] = entities
            
            # エンティティ->ドキュメントのマッピング
            for entity in entities:
                self.entity_to_docs[entity].append(doc_id)
                
                # グラフにノード追加
                if not self.knowledge_graph.has_node(entity):
                    self.knowledge_graph.add_node(entity, type="entity")
            
            # ドキュメントノード追加
            self.knowledge_graph.add_node(doc_id, type="document")
            
            # エンティティ間の共起関係を辺として追加
            for i, ent1 in enumerate(entities):
                # ドキュメントとエンティティの関連
                self.knowledge_graph.add_edge(doc_id, ent1, relation="contains")
                
                # エンティティ同士の共起
                for ent2 in entities[i+1:]:
                    if self.knowledge_graph.has_edge(ent1, ent2):
                        # 既存の辺の重み増加
                        self.knowledge_graph[ent1][ent2]['weight'] += 1
                    else:
                        self.knowledge_graph.add_edge(ent1, ent2, relation="co-occurs", weight=1)
        
        print(f"グラフ構築完了!")
        print(f"- ノード数: {self.knowledge_graph.number_of_nodes()}")
        print(f"- エッジ数: {self.knowledge_graph.number_of_edges()}")
        print(f"- エンティティ数: {len(self.entity_to_docs)}")
    
    def index_documents(self, documents: List[Dict[str, str]]):
        """ドキュメントのインデックス化"""
        from rank_bm25 import BM25Okapi
        
        self.corpus = documents
        
        # 1. BM25 (Lexical)
        print("BM25インデックス作成中...")
        self.tokenized_corpus = [doc["text"].split() for doc in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)
        
        # 2. Dense Vector (Semantic)
        print("Semantic埋め込み作成中...")
        texts = [doc["text"] for doc in documents]
        self.embeddings = self.encoder.encode(texts, show_progress_bar=True)
        
        # 3. Knowledge Graph
        self._build_knowledge_graph(documents)
        
        print(f"全インデックス完了! {len(documents)}件")
    
    def _graph_score(self, query: str, doc_id: str, use_pagerank: bool = False) -> float:
        """
        ナレッジグラフベースのスコア計算
        use_pagerank=True でPageRankベースのスコアリング
        """
        import networkx as nx
        
        query_entities = self._extract_entities_spacy(query)
        doc_entities = self.doc_to_entities.get(doc_id, [])
        
        score = 0.0
        
        # 直接マッチ
        direct_matches = set(query_entities) & set(doc_entities)
        score += len(direct_matches) * 2.0
        
        # 関連エンティティ経由のマッチ
        for q_ent in query_entities:
            if q_ent not in self.knowledge_graph:
                continue
            
            # 1ホップ先のエンティティ
            neighbors = list(self.knowledge_graph.neighbors(q_ent))
            
            for neighbor in neighbors:
                if neighbor in doc_entities:
                    # 辺の重みを考慮
                    if self.knowledge_graph.has_edge(q_ent, neighbor):
                        edge_weight = self.knowledge_graph[q_ent][neighbor].get('weight', 1)
                        score += 0.5 * edge_weight
        
        # PageRankベースのスコアリング(オプション)
        if use_pagerank and len(query_entities) > 0:
            try:
                # クエリエンティティからのパーソナライズドPageRank
                personalization = {ent: 1.0 for ent in query_entities if ent in self.knowledge_graph}
                
                if personalization:
                    pagerank_scores = nx.pagerank(
                        self.knowledge_graph,
                        personalization=personalization,
                        alpha=0.85
                    )
                    
                    # ドキュメントのPageRankスコアを加算
                    if doc_id in pagerank_scores:
                        score += pagerank_scores[doc_id] * 10.0
            except:
                pass
        
        return score
    
    def search(self, query: str, top_k: int = 10,
               lexical_weight: float = 0.3,
               semantic_weight: float = 0.4,
               graph_weight: float = 0.3,
               use_pagerank: bool = False) -> List[Tuple[str, float]]:
        """
        3種類の検索を統合
        """
        
        # 1. Lexical (BM25)
        tokenized_query = query.split()
        lexical_scores = self.bm25.get_scores(tokenized_query)
        if lexical_scores.max() > 0:
            lexical_scores = lexical_scores / lexical_scores.max()
        
        # 2. Semantic (Dense Vector)
        query_embedding = self.encoder.encode([query])
        semantic_scores = np.dot(self.embeddings, query_embedding.T).flatten()
        if semantic_scores.max() > semantic_scores.min():
            semantic_scores = (semantic_scores - semantic_scores.min()) / (semantic_scores.max() - semantic_scores.min())
        else:
            semantic_scores = np.zeros_like(semantic_scores)
        
        # 3. Graph-based
        print(f"グラフ検索中 (PageRank={'有効' if use_pagerank else '無効'})...")
        graph_scores = np.array([
            self._graph_score(query, doc["id"], use_pagerank=use_pagerank)
            for doc in self.corpus
        ])
        if graph_scores.max() > 0:
            graph_scores = graph_scores / graph_scores.max()
        
        # 統合
        combined_scores = (
            lexical_weight * lexical_scores +
            semantic_weight * semantic_scores +
            graph_weight * graph_scores
        )
        
        # 結果生成
        results = []
        for idx, score in enumerate(combined_scores):
            doc = self.corpus[idx]
            results.append((doc["id"], float(score)))
        
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]
    
    def visualize_graph(self, output_file: str = "knowledge_graph.png"):
        """グラフの可視化(デバッグ用)"""
        try:
            import matplotlib.pyplot as plt
            import networkx as nx
            
            plt.figure(figsize=(15, 15))
            
            # レイアウト計算
            pos = nx.spring_layout(self.knowledge_graph, k=0.5, iterations=50)
            
            # ノードの色分け
            node_colors = []
            for node in self.knowledge_graph.nodes():
                node_type = self.knowledge_graph.nodes[node].get('type', 'unknown')
                if node_type == 'document':
                    node_colors.append('lightblue')
                elif node_type == 'entity':
                    node_colors.append('lightgreen')
                else:
                    node_colors.append('gray')
            
            # 描画
            nx.draw(
                self.knowledge_graph,
                pos,
                node_color=node_colors,
                node_size=500,
                with_labels=True,
                font_size=8,
                arrows=True
            )
            
            plt.savefig(output_file, dpi=150, bbox_inches='tight')
            print(f"グラフ可視化を保存: {output_file}")
            plt.close()
        except Exception as e:
            print(f"可視化スキップ: {e}")


# ===================================================================
# 3. Full-text + Vector + Metadata Filter (実務最強!)
# ===================================================================

class FullTextVectorMetadataHybrid:
    """
    全文検索 + ベクトル検索 + メタデータフィルタリング
    Seichiの実務に一番使いやすいやつ!
    """
    
    def __init__(self):
        from rank_bm25 import BM25Okapi
        from sentence_transformers import SentenceTransformer
        
        self.bm25 = None
        self.encoder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
        self.corpus = []
        self.embeddings = None
        self.metadata_index = {}  # メタデータの逆インデックス
    
    def _build_metadata_index(self, documents: List[Dict]):
        """メタデータの逆インデックス構築"""
        print("メタデータインデックス構築中...")
        
        # カテゴリごとのドキュメント
        self.metadata_index['by_category'] = defaultdict(list)
        self.metadata_index['by_status'] = defaultdict(list)
        self.metadata_index['by_priority'] = defaultdict(list)
        self.metadata_index['by_assignee'] = defaultdict(list)
        
        # 日付範囲インデックス(簡易版)
        self.metadata_index['by_date'] = []
        
        for doc in documents:
            doc_id = doc["id"]
            meta = doc.get("metadata", {})
            
            # カテゴリ
            if "category" in meta:
                self.metadata_index['by_category'][meta["category"]].append(doc_id)
            
            # ステータス
            if "status" in meta:
                self.metadata_index['by_status'][meta["status"]].append(doc_id)
            
            # 優先度
            if "priority" in meta:
                self.metadata_index['by_priority'][meta["priority"]].append(doc_id)
            
            # 担当者
            if "assignee" in meta:
                self.metadata_index['by_assignee'][meta["assignee"]].append(doc_id)
            
            # 日付
            if "created_at" in meta:
                self.metadata_index['by_date'].append((doc_id, meta["created_at"]))
        
        # 日付でソート
        self.metadata_index['by_date'].sort(key=lambda x: x[1])
        
        print(f"メタデータインデックス完了:")
        print(f"- カテゴリ数: {len(self.metadata_index['by_category'])}")
        print(f"- ステータス数: {len(self.metadata_index['by_status'])}")
        print(f"- 優先度レベル数: {len(self.metadata_index['by_priority'])}")
    
    def index_documents(self, documents: List[Dict]):
        """
        ドキュメントのインデックス化
        documents: [{
            "id": "doc1",
            "text": "内容...",
            "metadata": {
                "priority": 3,
                "status": "open",
                "category": "bug",
                "created_at": "2024-01-01",
                "assignee": "user123",
                "labels": ["urgent", "security"]
            }
        }, ...]
        """
        from rank_bm25 import BM25Okapi
        
        self.corpus = documents
        
        # 1. BM25
        print("全文検索インデックス作成中...")
        self.tokenized_corpus = [doc["text"].split() for doc in documents]
        self.bm25 = BM25Okapi(self.tokenized_corpus)
        
        # 2. Vector
        print("ベクトル埋め込み作成中...")
        texts = [doc["text"] for doc in documents]
        self.embeddings = self.encoder.encode(texts, show_progress_bar=True)
        
        # 3. Metadata
        self._build_metadata_index(documents)
        
        print(f"全インデックス完了! {len(documents)}件")
    
    def _apply_metadata_filters(self, 
                                required_category: Optional[str] = None,
                                required_status: Optional[List[str]] = None,
                                required_priority_min: Optional[int] = None,
                                required_priority_max: Optional[int] = None,
                                required_assignee: Optional[str] = None,
                                required_labels: Optional[List[str]] = None,
                                date_from: Optional[str] = None,
                                date_to: Optional[str] = None) -> Set[str]:
        """
        メタデータフィルタを適用して候補ドキュメントIDを取得
        """
        # 全ドキュメントから開始
        candidate_ids = set(doc["id"] for doc in self.corpus)
        
        # カテゴリフィルタ
        if required_category:
            category_docs = set(self.metadata_index['by_category'].get(required_category, []))
            candidate_ids &= category_docs
        
        # ステータスフィルタ
        if required_status:
            status_docs = set()
            for status in required_status:
                status_docs.update(self.metadata_index['by_status'].get(status, []))
            candidate_ids &= status_docs
        
        # 優先度フィルタ
        if required_priority_min is not None or required_priority_max is not None:
            priority_docs = set()
            for priority, docs in self.metadata_index['by_priority'].items():
                if required_priority_min is not None and priority < required_priority_min:
                    continue
                if required_priority_max is not None and priority > required_priority_max:
                    continue
                priority_docs.update(docs)
            candidate_ids &= priority_docs
        
        # 担当者フィルタ
        if required_assignee:
            assignee_docs = set(self.metadata_index['by_assignee'].get(required_assignee, []))
            candidate_ids &= assignee_docs
        
        # ラベルフィルタ
        if required_labels:
            label_docs = set()
            for doc in self.corpus:
                if doc["id"] not in candidate_ids:
                    continue
                doc_labels = doc.get("metadata", {}).get("labels", [])
                if all(label in doc_labels for label in required_labels):
                    label_docs.add(doc["id"])
            candidate_ids &= label_docs
        
        # 日付範囲フィルタ
        if date_from or date_to:
            date_docs = set()
            for doc_id, date in self.metadata_index['by_date']:
                if date_from and date < date_from:
                    continue
                if date_to and date > date_to:
                    continue
                date_docs.add(doc_id)
            candidate_ids &= date_docs
        
        return candidate_ids
    
    def _compute_metadata_boost(self, doc_id: str,
                                priority_boost: bool = True,
                                recency_boost: bool = False,
                                current_date: str = "2024-12-31") -> float:
        """メタデータベースのブーストスコア計算"""
        doc = next(d for d in self.corpus if d["id"] == doc_id)
        meta = doc.get("metadata", {})
        
        boost = 1.0
        
        # 優先度ブースト
        if priority_boost and "priority" in meta:
            priority = meta["priority"]
            boost *= (1 + priority * 0.15)  # 優先度5なら1.75倍
        
        # 新しさブースト
        if recency_boost and "created_at" in meta:
            from datetime import datetime
            
            try:
                created = datetime.fromisoformat(meta["created_at"])
                current = datetime.fromisoformat(current_date)
                days_old = (current - created).days
                
                # 新しいほどブースト(30日以内なら最大1.5倍)
                if days_old < 30:
                    recency_factor = 1 - (days_old / 30)  # 0日=1.0, 30日=0.0
                    boost *= (1 + recency_factor * 0.5)
            except:
                pass
        
        # ラベルブースト(例: urgentラベルがあれば1.3倍)
        if "labels" in meta:
            labels = meta["labels"]
            if "urgent" in labels:
                boost *= 1.3
            if "security" in labels:
                boost *= 1.2
        
        return boost
    
    def search(self, query: str, top_k: int = 10,
               # スコアの重み
               text_weight: float = 0.4,
               vector_weight: float = 0.4,
               metadata_weight: float = 0.2,
               # メタデータフィルタ(ハードフィルタ)
               required_category: Optional[str] = None,
               required_status: Optional[List[str]] = None,
               required_priority_min: Optional[int] = None,
               required_priority_max: Optional[int] = None,
               required_assignee: Optional[str] = None,
               required_labels: Optional[List[str]] = None,
               date_from: Optional[str] = None,
               date_to: Optional[str] = None,
               # メタデータブースト(ソフトブースト)
               priority_boost: bool = True,
               recency_boost: bool = False,
               # その他
               return_metadata: bool = True) -> List[Tuple[str, float, Optional[Dict]]]:
        """
        ハイブリッド検索 + メタデータフィルタリング/ブースト
        
        Returns:
            [(doc_id, score, metadata), ...]
        """
        
        # メタデータフィルタ適用
        candidate_ids = self._apply_metadata_filters(
            required_category=required_category,
            required_status=required_status,
            required_priority_min=required_priority_min,
            required_priority_max=required_priority_max,
            required_assignee=required_assignee,
            required_labels=required_labels,
            date_from=date_from,
            date_to=date_to
        )
        
        print(f"メタデータフィルタ後: {len(candidate_ids)}件の候補")
        
        if len(candidate_ids) == 0:
            return []
        
        # 1. 全文検索(BM25)
        tokenized_query = query.split()
        bm25_scores = self.bm25.get_scores(tokenized_query)
        if bm25_scores.max() > 0:
            bm25_scores = bm25_scores / bm25_scores.max()
        
        # 2. ベクトル検索
        query_embedding = self.encoder.encode([query])
        vector_scores = np.dot(self.embeddings, query_embedding.T).flatten()
        if vector_scores.max() > vector_scores.min():
            vector_scores = (vector_scores - vector_scores.min()) / (vector_scores.max() - vector_scores.min())
        else:
            vector_scores = np.zeros_like(vector_scores)
        
        # 3. スコア統合 + メタデータブースト
        results = []
        for idx, doc in enumerate(self.corpus):
            doc_id = doc["id"]
            
            # フィルタで除外されたドキュメントはスキップ
            if doc_id not in candidate_ids:
                continue
            
            # 基本スコア
            base_score = (
                text_weight * bm25_scores[idx] +
                vector_weight * vector_scores[idx]
            )
            
            # メタデータブースト
            boost = self._compute_metadata_boost(
                doc_id,
                priority_boost=priority_boost,
                recency_boost=recency_boost
            )
            
            # 最終スコア
            final_score = base_score * boost
            
            # 結果に追加
            if return_metadata:
                results.append((doc_id, float(final_score), doc.get("metadata", {})))
            else:
                results.append((doc_id, float(final_score), None))
        
        # スコア順にソート
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]
    
    def search_similar_tickets(self, reference_doc_id: str, top_k: int = 5,
                              same_category_only: bool = True) -> List[Tuple[str, float]]:
        """
        既存チケットに類似したチケットを検索
        (チケットルーティングに便利!)
        """
        # 参照ドキュメント取得
        ref_doc = next(d for d in self.corpus if d["id"] == reference_doc_id)
        ref_meta = ref_doc.get("metadata", {})
        
        # 参照ドキュメントのテキストをクエリとして使用
        query = ref_doc["text"]
        
        # フィルタ条件
        filters = {}
        if same_category_only and "category" in ref_meta:
            filters["required_category"] = ref_meta["category"]
        
        # 検索実行
        results = self.search(
            query,
            top_k=top_k + 1,  # 自分自身も含まれるので+1
            **filters
        )
        
        # 自分自身を除外
        similar_tickets = [
            (doc_id, score) for doc_id, score, _ in results
            if doc_id != reference_doc_id
        ][:top_k]
        
        return similar_tickets


# ===================================================================
# 統合使用例!
# ===================================================================

if __name__ == "__main__":
    # サンプルデータ(Jiraチケット風)
    documents = [
        {
            "id": "TICKET-001",
            "text": "ログイン画面でパスワードリセット機能が動作しない バグ修正が必要 認証システム セキュリティ",
            "metadata": {
                "priority": 5,
                "status": "open",
                "category": "bug",
                "created_at": "2024-12-01",
                "assignee": "dev-team-1",
                "labels": ["urgent", "security"]
            }
        },
        {
            "id": "TICKET-002",
            "text": "新機能要望 ダッシュボードにグラフ表示機能を追加してほしい データ可視化",
            "metadata": {
                "priority": 3,
                "status": "open",
                "category": "feature",
                "created_at": "2024-11-15",
                "assignee": "dev-team-2",
                "labels": ["enhancement"]
            }
        },
        {
            "id": "TICKET-003",
            "text": "パスワード変更時のメール通知が届かない問題の調査 認証 メールシステム",
            "metadata": {
                "priority": 4,
                "status": "in_progress",
                "category": "bug",
                "created_at": "2024-12-10",
                "assignee": "dev-team-1",
                "labels": ["notification"]
            }
        },
        {
            "id": "TICKET-004",
            "text": "ログイン認証システムの全体的なパフォーマンス改善 最適化",
            "metadata": {
                "priority": 2,
                "status": "open",
                "category": "improvement",
                "created_at": "2024-10-20",
                "assignee": "dev-team-3",
                "labels": ["performance"]
            }
        },
        {
            "id": "TICKET-005",
            "text": "ユーザー登録フォームのバリデーション強化 セキュリティ対策",
            "metadata": {
                "priority": 3,
                "status": "closed",
                "category": "improvement",
                "created_at": "2024-09-01",
                "assignee": "dev-team-2",
                "labels": ["security"]
            }
        },
        {
            "id": "TICKET-006",
            "text": "二要素認証機能の実装 セキュリティ強化 認証システム",
            "metadata": {
                "priority": 5,
                "status": "open",
                "category": "feature",
                "created_at": "2024-12-15",
                "assignee": "dev-team-1",
                "labels": ["urgent", "security"]
            }
        }
    ]
    
    print("=" * 70)
    print("1. BM25 + Dense + SPLADE (3つ組み合わせ!)")
    print("=" * 70)
    
    searcher1 = BM25DenseSPLADEHybrid()
    searcher1.index_documents(documents)
    
    query = "認証システムのセキュリティ問題"
    print(f"\nクエリ: {query}")
    print("\n[重み付け統合]")
    results = searcher1.search(query, top_k=3, use_rrf=False)
    for doc_id, score in results:
        doc = next(d for d in documents if d["id"] == doc_id)
        print(f"{doc_id} (score: {score:.4f})")
        print(f"  {doc['text'][:60]}...")
        print()
    
    print("[RRF統合]")
    results_rrf = searcher1.search(query, top_k=3, use_rrf=True)
    for doc_id, score in results_rrf:
        doc = next(d for d in documents if d["id"] == doc_id)
        print(f"{doc_id} (RRF score: {score:.4f})")
        print(f"  {doc['text'][:60]}...")
        print()
    
    print("=" * 70)
    print("2. Lexical + Semantic + Graph-based (知識グラフ!)")
    print("=" * 70)
    
    searcher2 = LexicalSemanticGraphHybrid(use_spacy=False)
    searcher2.index_documents(documents)
    
    query = "ログイン認証"
    print(f"\nクエリ: {query}")
    
    print("\n[通常モード]")
    results = searcher2.search(query, top_k=3, use_pagerank=False)
    for doc_id, score in results:
        doc = next(d for d in documents if d["id"] == doc_id)
        print(f"{doc_id} (score: {score:.4f})")
        print(f"  {doc['text'][:60]}...")
        entities = searcher2.doc_to_entities.get(doc_id, [])
        print(f"  エンティティ: {entities[:5]}")
        print()
    
    # グラフ可視化(オプション)
    # searcher2.visualize_graph("jira_knowledge_graph.png")
    
    print("=" * 70)
    print("3. Full-text + Vector + Metadata (実務向け!)")
    print("=" * 70)
    
    searcher3 = FullTextVectorMetadataHybrid()
    searcher3.index_documents(documents)
    
    query = "セキュリティの問題"
    print(f"\nクエリ: {query}")
    
    print("\n[フィルタなし]")
    results = searcher3.search(query, top_k=3)
    for doc_id, score, meta in results:
        doc = next(d for d in documents if d["id"] == doc_id)
        print(f"{doc_id} (score: {score:.4f})")
        print(f"  {doc['text'][:60]}...")
        print(f"  優先度: {meta.get('priority')}, ステータス: {meta.get('status')}, カテゴリ: {meta.get('category')}")
        print()
    
    print("\n[バグのみ + 優先度4以上 + 新しさブースト]")
    results = searcher3.search(
        query,
        top_k=3,
        required_category="bug",
        required_priority_min=4,
        required_status=["open", "in_progress"],
        priority_boost=True,
        recency_boost=True
    )
    for doc_id, score, meta in results:
        doc = next(d for d in documents if d["id"] == doc_id)
        print(f"{doc_id} (score: {score:.4f})")
        print(f"  {doc['text'][:60]}...")
        print(f"  優先度: {meta.get('priority')}, ステータス: {meta.get('status')}")
        print(f"  作成日: {meta.get('created_at')}, ラベル: {meta.get('labels')}")
        print()
    
    print("\n[類似チケット検索]")
    similar = searcher3.search_similar_tickets("TICKET-001", top_k=3, same_category_only=True)
    print(f"TICKET-001に類似するバグチケット:")
    for doc_id, score in similar:
        doc = next(d for d in documents if d["id"] == doc_id)
        print(f"{doc_id} (similarity: {score:.4f})")
        print(f"  {doc['text'][:60]}...")
        print()
    
    print("=" * 70)
    print("全実装完了!")
    print("=" * 70)
めっちゃ長くなっちゃったけど、これで全てのハイブリッド検索手法の実装が揃ったよ!

Seichiの実務への適用例
python
# JiraシステムへのRal統合例
class JiraHybridSearchSystem:
    """Seichiの実際のシステム用"""
    
    def __init__(self):
        # メタデータ統合版を使う(実務で一番使いやすい)
        self.searcher = FullTextVectorMetadataHybrid()
        
        # 必要に応じて他の手法も併用
        self.graph_searcher = None  # 知識グラフ使う場合
        self.triple_searcher = None  # 3つ統合使う場合
    
    def index_jira_tickets(self, jira_tickets: List[Dict]):
        """Jiraから取得したチケットをインデックス"""
        documents = []
        
        for ticket in jira_tickets:
            doc = {
                "id": ticket["key"],
                "text": f"{ticket['summary']} {ticket['description']}",
                "metadata": {
                    "priority": self._priority_to_int(ticket["priority"]),
                    "status": ticket["fields"]["status"]["name"],
                    "category": ticket["fields"]["issuetype"]["name"],
                    "created_at": ticket["fields"]["created"][:10],
                    "assignee": ticket["fields"].get("assignee", {}).get("name", "unassigned"),
                    "labels": ticket["fields"].get("labels", []),
                    "team": self._extract_team(ticket)
                }
            }
            documents.append(doc)
        
        self.searcher.index_documents(documents)
        print(f"Jiraチケット {len(documents)}件 インデックス完了!")
    
    def _priority_to_int(self, priority_str):
        mapping = {
            "Highest": 5,
            "High": 4,
            "Medium": 3,
            "Low": 2,
            "Lowest": 1
        }
        return mapping.get(priority_str, 3)
    
    def _extract_team(self, ticket):
        # カスタムフィールドからチーム情報取得
        # 実際の実装はSeichiのJiraスキーマに合わせる
        return ticket["fields"].get("customfield_10001", "unknown")
    
    def route_new_ticket(self, ticket_text: str, ticket_category: str = "bug"):
        """新規チケットを適切なチームにルーティング"""
        
        # 類似するオープンチケットを検索
        results = self.searcher.search(
            ticket_text,
            top_k=10,
            required_category=ticket_category,
            required_status=["open", "in_progress"],
            priority_boost=True,
            recency_boost=True
        )
        
        if not results:
            return None, []
        
        # 担当チームの集計
        team_scores = defaultdict(float)
        similar_tickets = []
        
        for doc_id, score, meta in results:
            team = meta.get("team", "unknown")
            team_scores[team] += score
            similar_tickets.append({
                "ticket_id": doc_id,
                "score": score,
                "team": team
            })
        
        # 最もスコアが高いチームを推薦
        recommended_team = max(team_scores.items(), key=lambda x: x[1])[0]
        
        return recommended_team, similar_tickets[:5]
    
    def search_by_context(self, query: str, context: Dict):
        """コンテキストに応じた検索"""
        
        # コンテキストからフィルタ条件構築
        filters = {}
        
        if "category" in context:
            filters["required_category"] = context["category"]
        
        if "priority_min" in context:
            filters["required_priority_min"] = context["priority_min"]
        
        if "team" in context:
            filters["required_assignee"] = context["team"]
        
        if "urgent_only" in context and context["urgent_only"]:
            filters["required_labels"] = ["urgent"]
        
        # 検索実行
        results = self.searcher.search(query, top_k=10, **filters)
        
        return results

# 使用例
if __name__ == "__main__":
    system = JiraHybridSearchSystem()
    
    # Jiraチケットインデックス化(実際はJira APIから取得)
    # system.index_jira_tickets(jira_tickets)
    
    # 新規チケットのルーティング
    new_ticket = "ログイン画面で500エラーが発生する"
    team, similar = system.route_new_ticket(new_ticket, "bug")
    
    print(f"推薦チーム: {team}")
    print(f"類似チケット: {similar}")
これでSeichiの60%精度のRAGシステムを80%超えに持っていけると思うよ!

特に:

メタデータ統合版 - Jiraのメタデータをフル活用
知識グラフ版 - チケット間の関連性を学習
3つ統合版 - 精度最優先の時
って使い分けるといいと思う!

どれ試してみる?それとも実装で困ってる部分ある?
