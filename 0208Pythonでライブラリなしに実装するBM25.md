pythonã«ã¦bm25ã¯bm25ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã‚ãšã«å®Ÿè£…ã™ã‚‹ã“ã¨ã¯å¯èƒ½ã§ã™ã‹?

2æœˆ7æ—¥
ã‚„ã£ã»ãƒ¼!ã‚‚ã¡ã‚ã‚“å¯èƒ½ã ã‚ˆãƒ¼!ğŸ‰

BM25ã£ã¦çµæ§‹ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã ã‹ã‚‰ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ã‚ãªãã¦ã‚‚å…¨ç„¶å®Ÿè£…ã§ãã‚‹ã‚“ã ã‚ˆã­!

åŸºæœ¬çš„ãªæ•°å¼ã¯ã“ã‚“ãªæ„Ÿã˜â†“

python
import math
from collections import Counter

class BM25:
    def __init__(self, corpus, k1=1.5, b=0.75):
        """
        corpus: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ(å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯å˜èªã®ãƒªã‚¹ãƒˆ)
        k1: term frequencyã®é£½å’Œã‚’åˆ¶å¾¡ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        b: æ–‡æ›¸é•·ã®æ­£è¦åŒ–ã®åº¦åˆã„
        """
        self.corpus = corpus
        self.k1 = k1
        self.b = b
        self.doc_len = [len(doc) for doc in corpus]
        self.avgdl = sum(self.doc_len) / len(self.doc_len)
        self.doc_count = len(corpus)
        self.idf = self._calc_idf()
    
    def _calc_idf(self):
        """å„å˜èªã®IDFã‚’è¨ˆç®—"""
        idf = {}
        # å„å˜èªãŒä½•å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å‡ºç¾ã™ã‚‹ã‹ã‚«ã‚¦ãƒ³ãƒˆ
        df = Counter()
        for doc in self.corpus:
            for word in set(doc):
                df[word] += 1
        
        # IDFè¨ˆç®—: log((N - df + 0.5) / (df + 0.5) + 1)
        for word, freq in df.items():
            idf[word] = math.log((self.doc_count - freq + 0.5) / (freq + 0.5) + 1)
        
        return idf
    
    def get_scores(self, query):
        """ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        scores = []
        
        for idx, doc in enumerate(self.corpus):
            score = 0
            doc_len = self.doc_len[idx]
            word_freq = Counter(doc)
            
            for word in query:
                if word not in self.idf:
                    continue
                
                # å˜èªã®å‡ºç¾é »åº¦
                freq = word_freq.get(word, 0)
                
                # BM25ã‚¹ã‚³ã‚¢è¨ˆç®—
                numerator = freq * (self.k1 + 1)
                denominator = freq + self.k1 * (1 - self.b + self.b * (doc_len / self.avgdl))
                score += self.idf[word] * (numerator / denominator)
            
            scores.append(score)
        
        return scores

# ä½¿ã„æ–¹
corpus = [
    ["ã“ã‚“ã«ã¡ã¯", "ä¸–ç•Œ"],
    ["ã“ã‚“ã«ã¡ã¯", "Python"],
    ["BM25", "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ", "æ¤œç´¢"]
]

bm25 = BM25(corpus)
query = ["ã“ã‚“ã«ã¡ã¯", "Python"]
scores = bm25.get_scores(query)
print(scores)
ãƒã‚¤ãƒ³ãƒˆã¯ã­:

IDFè¨ˆç®—: ã©ã‚Œã ã‘ãƒ¬ã‚¢ãªå˜èªã‹ã‚’è©•ä¾¡
TFæ­£è¦åŒ–: æ–‡æ›¸ã®é•·ã•ã§æ­£è¦åŒ–ã—ã¦ã€é•·ã„æ–‡æ›¸ãŒæœ‰åˆ©ã«ãªã‚‰ãªã„ã‚ˆã†ã«ã™ã‚‹
k1ã¨b: ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§èª¿æ•´å¯èƒ½!
ã‚ã£ã¡ã‚ƒã‚·ãƒ³ãƒ—ãƒ«ã§ã—ã‚‡?ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãªã—ã§ã‚‚ä½™è£•ã§å®Ÿè£…ã§ãã¡ã‚ƒã†ğŸ’ªâœ¨

Seiichiãã‚“ãŒä½œã£ã¦ã‚‹RAGã‚·ã‚¹ãƒ†ãƒ ã«çµ„ã¿è¾¼ã‚€ãªã‚‰ã€ã“ã®å®Ÿè£…ã‚’ãƒ™ãƒ¼ã‚¹ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹ã®ã‚‚ã‚¢ãƒªã ã‚ˆã­!

ãŸã—ã‹bm25ã£ã¦ç´°ã‹ã„æ´¾ç”Ÿã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã£ã‘ï¼Ÿæœ«å°¾ã«LãŒã¤ãã¿ãŸã„ãªï¼Ÿ

0:28
ãŠã‰!ã‚ˆãçŸ¥ã£ã¦ã‚‹ã˜ã‚ƒã‚“!ãã†ãã†ã€BM25ã«ã¯ã„ãã¤ã‹æ´¾ç”Ÿãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒã‚ã‚‹ã‚“ã ã‚ˆã­!ğŸ˜Š

ä¸»ãªæ´¾ç”Ÿã¯ã“ã‚“ãªæ„Ÿã˜â†“

1. BM25L (BM25 with Length normalization)

æ–‡æ›¸é•·ã®æ­£è¦åŒ–ã‚’ã‚ˆã‚Šæ´—ç·´ã•ã›ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³
çŸ­ã„æ–‡æ›¸ãŒä¸åˆ©ã«ãªã‚‰ãªã„ã‚ˆã†ã«æ”¹è‰¯ã•ã‚Œã¦ã‚‹!
ãƒ‡ãƒ«ã‚¿(Î´)ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¦èª¿æ•´
2. BM25+ (BM25 Plus)

TFéƒ¨åˆ†ã«ä¸‹é™å€¤ã‚’è¨­å®šã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³
å˜èªãŒ1å›ã—ã‹å‡ºç¾ã—ãªãã¦ã‚‚ã‚ã‚‹ç¨‹åº¦ã‚¹ã‚³ã‚¢ãŒä»˜ãã‚ˆã†ã«ã—ãŸ
Î´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ä¸‹é™ã‚’èª¿æ•´
3. BM25-Adpt (BM25 Adaptive)

ã‚³ãƒ¼ãƒ‘ã‚¹å…¨ä½“ã®çµ±è¨ˆæƒ…å ±ã‚’ä½¿ã£ã¦å‹•çš„ã«èª¿æ•´
ã¡ãªã¿ã«BM25Lã®å®Ÿè£…ã¯ã“ã‚“ãªæ„Ÿã˜â†“

python
class BM25L:
    def __init__(self, corpus, k1=1.5, b=0.75, delta=0.5):
        self.corpus = corpus
        self.k1 = k1
        self.b = b
        self.delta = delta  # ã“ã‚ŒãŒè¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿!
        self.doc_len = [len(doc) for doc in corpus]
        self.avgdl = sum(self.doc_len) / len(self.doc_len)
        self.doc_count = len(corpus)
        self.idf = self._calc_idf()
    
    def _calc_idf(self):
        idf = {}
        df = Counter()
        for doc in self.corpus:
            for word in set(doc):
                df[word] += 1
        
        for word, freq in df.items():
            idf[word] = math.log((self.doc_count + 1) / (freq + 0.5))
        
        return idf
    
    def get_scores(self, query):
        scores = []
        
        for idx, doc in enumerate(self.corpus):
            score = 0
            doc_len = self.doc_len[idx]
            word_freq = Counter(doc)
            
            for word in query:
                if word not in self.idf:
                    continue
                
                freq = word_freq.get(word, 0)
                
                # BM25Lã®è¨ˆç®—å¼
                c_d = 1 - self.b + self.b * (doc_len / self.avgdl)
                numerator = (self.k1 + 1) * (freq + self.delta)
                denominator = self.k1 * c_d + freq + self.delta
                
                score += self.idf[word] * (numerator / denominator)
            
            scores.append(score)
        
        return scores
BM25+ã‚‚æ›¸ã„ã¨ãã­!

python
class BM25Plus:
    def __init__(self, corpus, k1=1.5, b=0.75, delta=1.0):
        self.corpus = corpus
        self.k1 = k1
        self.b = b
        self.delta = delta
        self.doc_len = [len(doc) for doc in corpus]
        self.avgdl = sum(self.doc_len) / len(self.doc_len)
        self.doc_count = len(corpus)
        self.idf = self._calc_idf()
    
    def _calc_idf(self):
        idf = {}
        df = Counter()
        for doc in self.corpus:
            for word in set(doc):
                df[word] += 1
        
        for word, freq in df.items():
            idf[word] = math.log((self.doc_count + 1) / freq)
        
        return idf
    
    def get_scores(self, query):
        scores = []
        
        for idx, doc in enumerate(self.corpus):
            score = 0
            doc_len = self.doc_len[idx]
            word_freq = Counter(doc)
            
            for word in query:
                if word not in self.idf:
                    continue
                
                freq = word_freq.get(word, 0)
                
                # BM25+ã®è¨ˆç®—å¼(ãƒ‡ãƒ«ã‚¿ã‚’åŠ ç®—!)
                numerator = freq * (self.k1 + 1)
                denominator = freq + self.k1 * (1 - self.b + self.b * (doc_len / self.avgdl))
                
                score += self.idf[word] * ((numerator / denominator) + self.delta)
            
            scores.append(score)
        
        return scores
å®Ÿéš›ã«ã©ã‚Œä½¿ã†ã‹ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç‰¹æ€§æ¬¡ç¬¬ã‹ãª!

çŸ­ã„æ–‡æ›¸ãŒå¤šã„â†’BM25L
å˜èªã®å‡ºç¾å›æ•°ãŒå°‘ãªã„â†’BM25+
æ¨™æº–çš„ãªã‚±ãƒ¼ã‚¹â†’æ™®é€šã®BM25ã§OK
Seiichiãã‚“ã®web scrapingãƒ‡ãƒ¼ã‚¿ã¨ã‹ã ã¨ã€ãƒšãƒ¼ã‚¸ã«ã‚ˆã£ã¦é•·ã•ãƒãƒ©ãƒãƒ©ã ã‚ã†ã‹ã‚‰ã€BM25Lè©¦ã—ã¦ã¿ã‚‹ã®ã‚‚ã‚¢ãƒªã‹ã‚‚ã­!âœ¨

ã¡ãªã¿ã«ã“ã®BM25Lã‚’ä½¿ãŠã†ã¨è€ƒãˆã¦ã„ã‚‹ã®ã¯RAGã‚·ã‚¹ãƒ†ãƒ ã§ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’å®Ÿè£…ã—ã‚ˆã†ã¨è€ƒãˆã¦ã„ã‚‹ãŸã‚ã§ã™ã€‚

0:30
ãŠã‰ãƒ¼!ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢!ã‚ã£ã¡ã‚ƒã„ã„ã˜ã‚ƒã‚“!ğŸ‰

RAGã‚·ã‚¹ãƒ†ãƒ ã§ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã£ã¦ã€BM25(ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹)ã¨ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢(ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢)ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã‚„ã¤ã ã‚ˆã­!ã“ã‚Œã€å˜ä½“ã§ä½¿ã†ã‚ˆã‚Šç²¾åº¦ã‚ã£ã¡ã‚ƒä¸ŠãŒã‚‹ã‚“ã ã‚ˆã­!ğŸ’ª

å…¸å‹çš„ãªå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã“ã‚“ãªæ„Ÿã˜â†“

python
import numpy as np
from sentence_transformers import SentenceTransformer

class HybridSearch:
    def __init__(self, corpus, tokenized_corpus, model_name='intfloat/multilingual-e5-small'):
        """
        corpus: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        tokenized_corpus: ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ(BM25ç”¨)
        """
        self.corpus = corpus
        
        # BM25Lã®åˆæœŸåŒ–
        self.bm25 = BM25L(tokenized_corpus)
        
        # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®æº–å‚™
        self.model = SentenceTransformer(model_name)
        self.doc_embeddings = self.model.encode(corpus, convert_to_numpy=True)
    
    def search(self, query, tokenized_query, top_k=5, bm25_weight=0.5):
        """
        ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢
        bm25_weight: BM25ã®ã‚¹ã‚³ã‚¢ã®é‡ã¿(0.0ã€œ1.0)
        """
        # BM25ã‚¹ã‚³ã‚¢å–å¾—
        bm25_scores = np.array(self.bm25.get_scores(tokenized_query))
        # æ­£è¦åŒ–(0ã€œ1ã®ç¯„å›²ã«)
        if bm25_scores.max() > 0:
            bm25_scores = bm25_scores / bm25_scores.max()
        
        # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¹ã‚³ã‚¢å–å¾—
        query_embedding = self.model.encode([query], convert_to_numpy=True)
        vector_scores = np.dot(self.doc_embeddings, query_embedding.T).squeeze()
        # æ­£è¦åŒ–
        if vector_scores.max() > 0:
            vector_scores = (vector_scores + 1) / 2  # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’0ã€œ1ã«
        
        # ã‚¹ã‚³ã‚¢ã®çµ±åˆ(weighted sum)
        hybrid_scores = (bm25_weight * bm25_scores + 
                        (1 - bm25_weight) * vector_scores)
        
        # Top-Kå–å¾—
        top_indices = np.argsort(hybrid_scores)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append({
                'doc': self.corpus[idx],
                'score': hybrid_scores[idx],
                'bm25_score': bm25_scores[idx],
                'vector_score': vector_scores[idx],
                'index': idx
            })
        
        return results


# ä½¿ç”¨ä¾‹
corpus = [
    "Pythonã§RAGã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹æ–¹æ³•",
    "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨BM25ã®çµ„ã¿åˆã‚ã›",
    "æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³"
]

# ãƒˆãƒ¼ã‚¯ãƒ³åŒ–(ç°¡æ˜“ç‰ˆã€å®Ÿéš›ã¯MeCabã¨ã‹ä½¿ã†)
tokenized_corpus = [doc.split() for doc in corpus]

searcher = HybridSearch(corpus, tokenized_corpus)

query = "RAGã‚·ã‚¹ãƒ†ãƒ ã®æ¤œç´¢æ–¹æ³•"
tokenized_query = query.split()

results = searcher.search(query, tokenized_query, top_k=3, bm25_weight=0.5)

for r in results:
    print(f"Score: {r['score']:.3f} (BM25: {r['bm25_score']:.3f}, Vector: {r['vector_score']:.3f})")
    print(f"Doc: {r['doc']}\n")
ã‚‚ã£ã¨é«˜åº¦ãªã‚„ã‚Šæ–¹ã‚‚ã‚ã‚‹ã‚ˆ!

1. Reciprocal Rank Fusion (RRF)

python
def reciprocal_rank_fusion(bm25_scores, vector_scores, k=60):
    """
    RRFã§ã‚¹ã‚³ã‚¢ã‚’çµ±åˆ
    å„æ¤œç´¢çµæœã®ãƒ©ãƒ³ã‚¯ã‚’ä½¿ã£ã¦çµ±åˆã™ã‚‹æ–¹æ³•
    """
    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å–å¾—
    bm25_ranks = np.argsort(np.argsort(bm25_scores)[::-1])
    vector_ranks = np.argsort(np.argsort(vector_scores)[::-1])
    
    # RRFã‚¹ã‚³ã‚¢è¨ˆç®—
    rrf_scores = (1 / (k + bm25_ranks) + 1 / (k + vector_ranks))
    
    return rrf_scores
2. å‹•çš„ãªé‡ã¿èª¿æ•´

python
class AdaptiveHybridSearch(HybridSearch):
    def search(self, query, tokenized_query, top_k=5):
        bm25_scores = np.array(self.bm25.get_scores(tokenized_query))
        query_embedding = self.model.encode([query], convert_to_numpy=True)
        vector_scores = np.dot(self.doc_embeddings, query_embedding.T).squeeze()
        
        # ã‚¯ã‚¨ãƒªã®ç‰¹æ€§ã«å¿œã˜ã¦é‡ã¿ã‚’èª¿æ•´
        # çŸ­ã„ã‚¯ã‚¨ãƒªâ†’BM25é‡è¦–ã€é•·ã„ã‚¯ã‚¨ãƒªâ†’ãƒ™ã‚¯ãƒˆãƒ«é‡è¦–
        query_len = len(tokenized_query)
        if query_len <= 3:
            bm25_weight = 0.7  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’é‡è¦–
        else:
            bm25_weight = 0.3  # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚’é‡è¦–
        
        # ä»¥ä¸‹åŒæ§˜...
Seiichiãã‚“ã®å ´åˆã€Jiraé€£æºã®RAGã‚·ã‚¹ãƒ†ãƒ ã ã¨:

å°‚é–€ç”¨èªã‚„å›ºæœ‰åè©ãŒå¤šã„â†’BM25ã®é‡ã¿ã‚’é«˜ã‚ã«
æ–‡è„ˆç†è§£ãŒé‡è¦â†’ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®é‡ã¿ã‚’é«˜ã‚ã«
ã£ã¦æ„Ÿã˜ã§èª¿æ•´ã™ã‚‹ã¨ã„ã„ã‹ã‚‚!

å®Ÿéš›ã«60%ã®ç²¾åº¦ã‚’ã‚‚ã£ã¨ä¸Šã’ãŸã„ãªã‚‰ã€ã“ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã¯çµæ§‹åŠ¹ãã¨æ€ã†ã‚ˆ!âœ¨

ã©ã‚“ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è©¦ã™äºˆå®š?ãƒã‚±ãƒƒãƒˆã®æœ¬æ–‡ã¨ã‹?ğŸ¤”

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯Jiraãƒã‚±ãƒƒãƒˆ,ã«ãªã‚Šã¾ã™ã€‚

0:32
ãªã‚‹ã»ã©ã­!Jiraãƒã‚±ãƒƒãƒˆã‹ãƒ¼!ãã‚Œãªã‚‰ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚ã£ã¡ã‚ƒåŠ¹æœçš„ã ã¨æ€ã†!ğŸ˜Š

Jiraãƒã‚±ãƒƒãƒˆã£ã¦ç‰¹æ€§çš„ã«:

å›ºæœ‰åè©å¤šã„(ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã€ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåã€ãƒ¦ãƒ¼ã‚¶ãƒ¼å)
å°‚é–€ç”¨èªãƒ»ç•¥èªãŒé »å‡º(APIã€DBã€ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ã¨ã‹)
å®šå‹æ–‡ã¨ãƒ•ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆãŒæ··åœ¨
ãƒã‚±ãƒƒãƒˆç•ªå·ã§ã®å‚ç…§ãŒå¤šã„(PROJ-123ã¿ãŸã„ãª)
ã ã‹ã‚‰ã€BM25ã ã‘ã ã¨æ–‡è„ˆç†è§£å¼±ã„ã—ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã ã‘ã ã¨å›ºæœ‰åè©ã«å¼±ã„ã‚“ã ã‚ˆã­ğŸ’¦

Jiraãƒã‚±ãƒƒãƒˆå‘ã‘ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢å®Ÿè£…ã€ã“ã‚“ãªæ„Ÿã˜ã§ã©ã†?â†“

python
import re
from collections import Counter
import numpy as np
from sentence_transformers import SentenceTransformer
import MeCab

class JiraHybridSearch:
    def __init__(self, tickets, model_name='intfloat/multilingual-e5-small'):
        """
        tickets: Jiraãƒã‚±ãƒƒãƒˆã®ãƒªã‚¹ãƒˆ
        å„ãƒã‚±ãƒƒãƒˆã¯è¾æ›¸å½¢å¼: {'key': 'PROJ-123', 'summary': '...', 'description': '...', ...}
        """
        self.tickets = tickets
        self.wakati = MeCab.Tagger('-Owakati')
        
        # ãƒã‚±ãƒƒãƒˆãƒ†ã‚­ã‚¹ãƒˆã®æº–å‚™
        self.corpus = self._prepare_corpus()
        self.tokenized_corpus = self._tokenize_corpus()
        
        # BM25Lã®åˆæœŸåŒ–
        self.bm25 = BM25L(self.tokenized_corpus)
        
        # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®æº–å‚™
        self.model = SentenceTransformer(model_name)
        self.doc_embeddings = self.model.encode(self.corpus, convert_to_numpy=True)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹(ãƒã‚±ãƒƒãƒˆç•ªå·ãªã©ã§ã®æ¤œç´¢ç”¨)
        self.metadata_index = self._build_metadata_index()
    
    def _prepare_corpus(self):
        """ãƒã‚±ãƒƒãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        corpus = []
        for ticket in self.tickets:
            # summary + description ã‚’çµåˆ
            text = f"{ticket.get('summary', '')} {ticket.get('description', '')}"
            corpus.append(text)
        return corpus
    
    def _tokenize_corpus(self):
        """ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–(å›ºæœ‰è¡¨ç¾ã‚’ä¿è­·)"""
        tokenized = []
        for text in self.corpus:
            # ãƒã‚±ãƒƒãƒˆç•ªå·ã‚’ä¿è­·
            text = self._protect_ticket_numbers(text)
            # è‹±æ•°å­—ã®é€£ç¶šã‚‚ä¿è­·(APIã‚­ãƒ¼ã€ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ãªã©)
            tokens = self._tokenize_with_protection(text)
            tokenized.append(tokens)
        return tokenized
    
    def _protect_ticket_numbers(self, text):
        """ãƒã‚±ãƒƒãƒˆç•ªå·ã‚’ç‰¹åˆ¥ãƒˆãƒ¼ã‚¯ãƒ³ã«ç½®ãæ›ãˆ"""
        # PROJ-123 ã®ã‚ˆã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³
        return re.sub(r'([A-Z]+-\d+)', r'__TICKET__\1__', text)
    
    def _tokenize_with_protection(self, text):
        """ä¿è­·ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç¶­æŒã—ãªãŒã‚‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–"""
        # ä¿è­·ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¸€æ™‚çš„ã«ç½®ãæ›ãˆ
        protected_tokens = []
        protected_pattern = r'__TICKET__[A-Z]+-\d+__'
        
        def replace_protected(match):
            protected_tokens.append(match.group(0))
            return f' __PROTECTED_{len(protected_tokens)-1}__ '
        
        text = re.sub(protected_pattern, replace_protected, text)
        
        # MeCabã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        tokens = self.wakati.parse(text).strip().split()
        
        # ä¿è­·ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å¾©å…ƒ
        restored_tokens = []
        for token in tokens:
            match = re.match(r'__PROTECTED_(\d+)__', token)
            if match:
                idx = int(match.group(1))
                original = protected_tokens[idx].replace('__TICKET__', '').replace('__', '')
                restored_tokens.append(original)
            else:
                restored_tokens.append(token)
        
        return restored_tokens
    
    def _build_metadata_index(self):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ§‹ç¯‰"""
        index = {
            'ticket_key': {},
            'assignee': {},
            'status': {},
            'priority': {}
        }
        
        for idx, ticket in enumerate(self.tickets):
            # ãƒã‚±ãƒƒãƒˆç•ªå·
            if 'key' in ticket:
                index['ticket_key'][ticket['key']] = idx
            
            # æ‹…å½“è€…
            if 'assignee' in ticket:
                assignee = ticket['assignee']
                if assignee not in index['assignee']:
                    index['assignee'][assignee] = []
                index['assignee'][assignee].append(idx)
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
            if 'status' in ticket:
                status = ticket['status']
                if status not in index['status']:
                    index['status'][status] = []
                index['status'][status].append(idx)
            
            # å„ªå…ˆåº¦
            if 'priority' in ticket:
                priority = ticket['priority']
                if priority not in index['priority']:
                    index['priority'][priority] = []
                index['priority'][priority].append(idx)
        
        return index
    
    def search(self, query, top_k=5, filters=None, bm25_weight=0.5):
        """
        ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢
        filters: {'status': 'Open', 'priority': 'High'} ã®ã‚ˆã†ãªå½¢å¼
        """
        # ãƒã‚±ãƒƒãƒˆç•ªå·ã§ã®å®Œå…¨ä¸€è‡´æ¤œç´¢
        ticket_match = re.search(r'([A-Z]+-\d+)', query)
        if ticket_match:
            ticket_key = ticket_match.group(1)
            if ticket_key in self.metadata_index['ticket_key']:
                idx = self.metadata_index['ticket_key'][ticket_key]
                return [{
                    'ticket': self.tickets[idx],
                    'score': 1.0,
                    'match_type': 'exact_ticket_number'
                }]
        
        # ã‚¯ã‚¨ãƒªã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        protected_query = self._protect_ticket_numbers(query)
        tokenized_query = self._tokenize_with_protection(protected_query)
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯¾è±¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        valid_indices = self._apply_filters(filters)
        
        # BM25ã‚¹ã‚³ã‚¢
        bm25_scores = np.array(self.bm25.get_scores(tokenized_query))
        if bm25_scores.max() > 0:
            bm25_scores = bm25_scores / bm25_scores.max()
        
        # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¹ã‚³ã‚¢
        query_embedding = self.model.encode([query], convert_to_numpy=True)
        vector_scores = np.dot(self.doc_embeddings, query_embedding.T).squeeze()
        if vector_scores.max() > vector_scores.min():
            vector_scores = (vector_scores - vector_scores.min()) / (vector_scores.max() - vector_scores.min())
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ã‚³ã‚¢
        hybrid_scores = bm25_weight * bm25_scores + (1 - bm25_weight) * vector_scores
        
        # ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
        if valid_indices is not None:
            mask = np.zeros(len(hybrid_scores), dtype=bool)
            mask[valid_indices] = True
            hybrid_scores = np.where(mask, hybrid_scores, -np.inf)
        
        # Top-Kå–å¾—
        top_indices = np.argsort(hybrid_scores)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            if hybrid_scores[idx] == -np.inf:
                continue
            results.append({
                'ticket': self.tickets[idx],
                'score': hybrid_scores[idx],
                'bm25_score': bm25_scores[idx],
                'vector_score': vector_scores[idx],
                'index': idx
            })
        
        return results
    
    def _apply_filters(self, filters):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨"""
        if not filters:
            return None
        
        valid_indices = set(range(len(self.tickets)))
        
        for key, value in filters.items():
            if key in self.metadata_index and value in self.metadata_index[key]:
                if isinstance(self.metadata_index[key][value], list):
                    valid_indices &= set(self.metadata_index[key][value])
                else:
                    valid_indices &= {self.metadata_index[key][value]}
        
        return list(valid_indices) if valid_indices else []


# ä½¿ç”¨ä¾‹
tickets = [
    {
        'key': 'PROJ-123',
        'summary': 'ãƒ­ã‚°ã‚¤ãƒ³æ©Ÿèƒ½ã®ãƒã‚°ä¿®æ­£',
        'description': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒ­ã‚°ã‚¤ãƒ³ã§ããªã„å•é¡Œã‚’ä¿®æ­£ã™ã‚‹å¿…è¦ãŒã‚ã‚‹',
        'status': 'Open',
        'priority': 'High',
        'assignee': 'ä½è—¤'
    },
    {
        'key': 'PROJ-124',
        'summary': 'APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è¿½åŠ ',
        'description': 'æ–°ã—ã„REST APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å®Ÿè£…',
        'status': 'In Progress',
        'priority': 'Medium',
        'assignee': 'ç”°ä¸­'
    }
]

searcher = JiraHybridSearch(tickets)

# æ¤œç´¢ä¾‹1: é€šå¸¸æ¤œç´¢
results = searcher.search('ãƒ­ã‚°ã‚¤ãƒ³ã®å•é¡Œ', top_k=3)

# æ¤œç´¢ä¾‹2: ãƒ•ã‚£ãƒ«ã‚¿ä»˜ãæ¤œç´¢
results = searcher.search(
    'APIå®Ÿè£…',
    filters={'status': 'In Progress'},
    bm25_weight=0.6
)

# æ¤œç´¢ä¾‹3: ãƒã‚±ãƒƒãƒˆç•ªå·ã§ã®æ¤œç´¢
results = searcher.search('PROJ-123')
ã•ã‚‰ã«ç²¾åº¦ã‚’ä¸Šã’ã‚‹ãŸã‚ã®Tips!

python
# 1. Rerankingã‚’è¿½åŠ 
from sentence_transformers import CrossEncoder

class JiraHybridSearchWithRerank(JiraHybridSearch):
    def __init__(self, tickets, model_name='intfloat/multilingual-e5-small'):
        super().__init__(tickets, model_name)
        # Rerankãƒ¢ãƒ‡ãƒ«(ã‚ˆã‚Šç²¾åº¦é«˜ã„)
        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    
    def search(self, query, top_k=5, filters=None, bm25_weight=0.5, rerank_top_n=20):
        # ã¾ãšå€™è£œã‚’å¤šã‚ã«å–å¾—
        candidates = super().search(query, top_k=rerank_top_n, filters=filters, bm25_weight=bm25_weight)
        
        # Rerankingã§ç²¾åº¦å‘ä¸Š
        if len(candidates) > top_k:
            pairs = [(query, self.corpus[c['index']]) for c in candidates]
            rerank_scores = self.reranker.predict(pairs)
            
            for idx, candidate in enumerate(candidates):
                candidate['rerank_score'] = rerank_scores[idx]
            
            # Rerankã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
            candidates.sort(key=lambda x: x['rerank_score'], reverse=True)
            candidates = candidates[:top_k]
        
        return candidates
é‡ã¿èª¿æ•´ã®æˆ¦ç•¥

python
# ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—åˆ¥ã®é‡ã¿èª¿æ•´
def adaptive_weight(query):
    # ãƒã‚±ãƒƒãƒˆç•ªå·å‚ç…§ãŒå¤šã„â†’BM25é‡è¦–
    if re.search(r'[A-Z]+-\d+', query):
        return 0.7
    
    # çŸ­ã„ã‚¯ã‚¨ãƒª(ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢çš„)â†’BM25é‡è¦–
    if len(query.split()) <= 3:
        return 0.65
    
    # é•·ã„æ–‡ç« (æ„å‘³ç†è§£ãŒé‡è¦)â†’ãƒ™ã‚¯ãƒˆãƒ«é‡è¦–
    if len(query.split()) >= 10:
        return 0.3
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    return 0.5
Jiraãƒã‚±ãƒƒãƒˆã ã¨ã€ã“ã†ã„ã†å·¥å¤«ã§ç²¾åº¦ã‹ãªã‚Šä¸ŠãŒã‚‹ã¨æ€ã†ã‚ˆ!

ä»Šã®60%ç²¾åº¦ã‹ã‚‰ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰+Rerankingã§75-80%ãã‚‰ã„ã¾ã§ç‹™ãˆã‚‹ã‚“ã˜ã‚ƒãªã„ã‹ãª?âœ¨

ä½•ã‹è³ªå•ã‚ã‚‹?å®Ÿè£…ã§è©°ã¾ã‚Šãã†ãªã¨ã“ã‚ã¨ã‹!ğŸ¤”
